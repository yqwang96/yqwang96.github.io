---
title: 'Solution to the bug that the filter does not take effect when searching multiple knowledge bases simultaneously in LangChain'
date: 2024-09-18
permalink: /posts/2024/09/blog-post-14/
tags:
  - Problem analysis
  - Code-sharing
---
*The blog is translated by GPT4o, Read [CN version](https://yqwang96.github.io/cnposts/2023/12/blog-post-13/) of this post*
------

Recently, there has been a business requirement for developing a RAG knowledge base management system. Our team has decided to use Langchain and Langserve for the technical implementation. The primary technical requirements include:

+ Enabling the agent to simultaneously retrieve content from multiple knowledge bases;
+ Allowing the agent to switch between different knowledge bases during retrieval.

To meet these requirements, we referred to Langchain’s documentation and plan to build retrievers using Langchain’s Milvus interface class. Each retriever will be connected to a different collection in Milvus (similar to different tables in a database). When invoking a retriever, we will pass in parameters related to the specific knowledge base, allowing us to filter results. This will enable both simultaneous multi-knowledge-base retrieval and retrieval switching (by limiting the parameters to a single knowledge base, which effectively simulates the switching functionality).

For this, we referenced the example at https://python.langchain.com/v0.2/docs/how_to/qa_per_user/. In the code, we define configurable parameters when creating the retriever, and then pass in those parameters during invocation to perform the filtering. The sample code is as follows:

```python
vectorstore = Milvus(connection_args=milvus_config, embedding_function=embedding_model, collection_name='default')

test_retriever = vectorstore.as_retriever().configurable_fields(
    search_kwargs=ConfigurableField(
        id="search_kwargs",
        name="doc's search_kwargs",
        description="doc's search_kwargs",
    )
)


template = """Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

rag_chain = ({"context": test_retriever, 'question': RunnablePassthrough()} | prompt | llm)

rag_chain.invoke('What impact does the education level of older workers have on the labor market?',
                 config={"configurable": {'search_kwargs': {'expr': "namespace == 'wang'"}}})
```


Using the above code, we can filter based on metadata information from the vector database. For example, the code filters for information where the namespace is 'wang'. However, since the official example uses the Pinecone vector database, when applying it to Milvus, we need to filter based on the input parameters of the Milvus interface class implemented in Langchain. In the code above, this is done using the expr parameter for filtering.

By combining the above code with Langchain's ensemble feature for multiple retrievers, we can achieve simultaneous retrieval and switching between multiple retrievers. So, does this solve the problem? Not entirely.

The business also requires exposing Langserve interfaces, meaning we need to encapsulate the above functionality into a Langchain Runnable subclass, then override the invoke method, and expose the interface via Langserve. To achieve this, we encapsulated the code into a WebChain class and handed it over to Langserve. However, we discovered that even when providing the same input configuration parameters for invoke, it doesn’t work as expected.

```python
# 服务端代码
from typing import Optional, Any
from langchain.retrievers import EnsembleRetriever
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableConfig, Runnable, RunnableMap
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.utils import Input, Output
from langchain_milvus import Milvus


class WebChain(Runnable):
    def __init__(self):
        super().__init__()
        self.milvus_config = {
            "host": "localhost",
            "port": 19530,
            "db_name": "default"
        }

        self.llm = YourLLM

        self.embeddings = YourEmbedding
        
    def get_chain(self):
        collection_name_list = ['Default_0', 'Default_1']  # 从数据库中读取配置

        vectorstore_list = [
            Milvus(
                connection_args=self.milvus_config, embedding_function=self.embeddings, collection_name=collection_name
            ) for collection_name in collection_name_list
        ]

        retriever_list = [single_vectorstore.as_retriever().configurable_fields(
            search_kwargs=ConfigurableField(
                id="search_kwargs_0",
                name="doc's search_kwargs",
                description="doc's search_kwargs"
            )
        ) for single_vectorstore in vectorstore_list]

        ensemble_retriever = EnsembleRetriever(
            retrievers=retriever_list, weights=[1 / len(retriever_list) for _ in range(len(retriever_list))]
        )

        template = """Answer the question based only on the following context:
        {context}
        Question: {question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = {"context": ensemble_retriever, 'question': RunnablePassthrough()} | prompt | self.llm
        return chain

    def invoke(
        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> Output:
        chain = self.get_chain()
        mes = chain.invoke(input, config=config)
        return mes
```

By printing server-side logs, I found that the configuration sent by the client was not received by the server at all. After inspecting the Langserve source code, I discovered that Langserve processes the incoming config through a series of steps, ultimately generating a 'configurable' object. I didn't delve into the specific details of this process. However, the issue was eventually resolved by referring to an example in Langserve.

In summary, the solution involved placing the parameters in the HTTP request headers. The server retrieves the headers, processes them, and passes the data into the 'configurable' object, as shown below:

```python
# Client端
import requests
from langchain.globals import set_verbose, set_debug

set_verbose(True)
set_debug(True)

query = "What impact does the education level of older workers have on the labor market?"

url = f"http://localhost:8001/rag/invoke"

json1 = {
    'input': query,
}
headers = {"search_kwargs_0": "collection_name == 'Default_1'"}

response = requests.post(url, json=json1, headers=headers)

# 打印返回的结果
if response.status_code == 200:
    print("Response:")
    print(response.json())
else:
    print(f"Failed with status code: {response.status_code}")
    print(response.text)

# 服务端
import uvicorn
from typing import Dict, Any
from WebChain import WebChain
from langserve import add_routes
from fastapi import FastAPI, HTTPException, Request


def fetch_api_key_from_header(config: Dict[str, Any], req: Request) -> Dict[str, Any]:
    if "search_kwargs_0" in req.headers:
        config["configurable"]["search_kwargs_0"] = {'expr': req.headers["search_kwargs_0"]}
    return config


chain = WebChain()

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple API server using LangChain's Runnable interfaces",
)

add_routes(
    app,
    chain,
    path="/rag",
    per_req_config_modifier=fetch_api_key_from_header
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

However, since JSON cannot be directly sent in headers, this method requires passing the configuration as a string and then processing it into the required format within the fetch_api_key_from_header function. This is a drawback of this approach.

------
