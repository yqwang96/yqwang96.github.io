---
title: 'The summary of connecting Ragflow to Langchain and Milvus'
date: 2024-09-22
permalink: /posts/2024/09/blog-post-16/
tags:
  - Bug analysis
  - Code-sharing
---

Based on the previously mentioned development requirements for RAG knowledge base management, after last Friday's technical discussion, we decided to first try integrating the existing LangChain code into the Ragflow interface. This approach can reduce development demands and avoid reinventing the wheel. Today, I spent the day researching at home, encountered some challenges, and managed to write a demo that successfully runs the overall process. I'm documenting this for reference for others with similar needs, as well as for future review.

------
*The blog is translated by GPT4o, Read [CN version](https://yqwang96.github.io//cnposts/2024/09/blog-post-16/) of this post*

I won't go into too much detail about Ragflow and LangChain here. Interested readers can refer to the [RAGFlow GitHub repository](https://github.com/infiniflow/ragflow) and the [LangChain documentation](https://python.langchain.com/v0.2/docs/introduction/). Regarding the current development requirements, I have already implemented knowledge base switching management functionality both with and without using Langserve. The next task is to integrate this functionality into Ragflow.

During my research on implementing Langserve, I pulled the Milvus image in Docker and created a container for testing. My Ragflow setup is also deployed via Docker by pulling the image and creating a container. Therefore, to integrate Ragflow and LangChain, we need to combine these two images together. This was the first challenge I encountered.

## Integration of Ragflow and Milvus Environment

During the integration of the Ragflow and Milvus environments, a key issue that arose was how to handle the duplicate MinIO images present in both image groups. The initial plan was to write a separate Dockerfile to merge the two image groups while retaining only the MinIO from the Milvus image to avoid duplication.

Although I successfully pulled the merged image and created a container, a serious problem emerged during runtime: the code could not connect to Milvus, preventing the data from being vectorized and stored. Additionally, the front-end visualization software for the vector knowledge base was also non-functional. While the Milvus container could start, it automatically shut down each time a connection attempt was made. Even checking the logs did not reveal any clear error messages.

### Troubleshooting Process
Initially, I suspected a configuration issue with MinIO or etcd, which might have prevented Milvus from communicating properly with these components, triggering an automatic shutdown mechanism. I made several configuration adjustments, but the issue remained unresolved. Unable to pinpoint a clear cause of the error, I ultimately abandoned this integration approach.

### Shift in Strategy
I then decided to take a different approach: I would pull the Milvus and Ragflow images separately, stopping the MinIO service in Ragflow and only running the MinIO from the Milvus image. The key focus in this strategy was to ensure that Ragflow and Milvus could communicate effectively within the same network. To achieve this, the Docker Compose file needed to explicitly specify that the two containers share the same network, as shown below:

```yml
version: '3'
services:
  milvus-standalone:
    image: milvusdb/milvus-standalone:latest
    container_name: milvus
    networks:
      - shared_network
    environment:
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=your_access_key
      - MINIO_SECRET_KEY=your_secret_key
    ports:
      - "19530:19530"
      - "9000:9000"

  ragflow-server:
    image: your_ragflow_image
    container_name: ragflow
    networks:
      - shared_network
    depends_on:
      - milvus-standalone
    environment:
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=your_access_key
      - MINIO_SECRET_KEY=your_secret_key
    ports:
      - "8080:8080"

networks:
  shared_network:
    driver: bridge
```

## Ragflow Chat Interface Calling Process

When testing the Ragflow chat interface, the process is as follows:

1. First, I checked the call logs of the Ragflow container and discovered that messages are sent via an interface called `completion`. I then searched for this interface in PyCharm.
2. After locating the interface, I found that it calls the `Chat` interface within the `dialog_service`. The next step is to obtain the user's question from the messenger and return it in the format required by Ragflow using the `Chat` interface.
3. In the subsequent process, I needed to paste the code and install some necessary libraries using `pip install`. Additionally, during debugging, I found that using `print` statements did not output debug information. To resolve this, I utilized the `chat_logger` variable to log relevant information with `chat_logger.error`, saving these error messages to a TXT file for later review of each step's execution.


## Detailed Debugging Process

After clarifying the previous section, to proceed with further debugging, the first step is to store the text in the database. This is done by accessing the exposed port locally for data entry. Initially, I used the `huggingfaces/sentence-transformer` for this purpose. At the same time, I employed this embedding model in the Retriever to vectorize the query and extract similar texts.

The issue arose because this model was loaded locally. Although its parameters are not extensive, the loading process still takes time, causing the Ragflow backend to hang during execution. To address this, I considered utilizing some mature APIs since they wouldnâ€™t require additional deployment and could perform embeddings more efficiently.

During subsequent execution, I realized that I had only modified the embedding model in the Retriever without updating the embedding model during data entry, which led to some errors. Therefore, it was necessary to re-vectorize and store the data with the newly adopted embedding model. After completing the vectorization, I continuously logged information in Ragflow to observe the state of each variable for debugging purposes.

After multiple adjustments to the response format, Ragflow was finally able to return the extracted data correctly. For filtering the knowledge base, I could use the initial configuration method without Langserve.

If further filtering is needed later, relevant information can be retrieved from the database using the knowledge base ID in Ragflow's dialog. This information can be used to construct a JSON config that will ultimately be passed to achieve filtering and responses.

The overall process is outlined above. Detailed code can be found in the content below.


```python
def chat(dialog, messages, stream=True, **kwargs):
    chat_logger.error('test chat!!!!!!!!!!!!!!!!!!!!!!!')
    import os
    import getpass
    from langchain_community.embeddings import ZhipuAIEmbeddings
    from typing import Optional, Any, Type, List, Dict
    from langchain.retrievers import EnsembleRetriever
    from langchain_core.runnables import ConfigurableField
    from langchain_openai import ChatOpenAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.runnables import RunnableConfig, Runnable, RunnableBinding, RunnablePassthrough
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables.utils import Input, Output
    from langchain_milvus import Milvus

    config = {
    'configurable': {
         "search_kwargs_0": {
             "expr": f"collection_name in ['Default_1']"
         }
     }
    }

    if not os.getenv("ZHIPUAI_API_KEY"):
        os.environ["ZHIPUAI_API_KEY"] = 'xxx.xxx'

    milvus_config = {
        "host": "172.19.0.8",
        "port": 19530,
        "db_name": "default"
    }

    llm = ChatOpenAI(
        model='deepseek-chat', openai_api_key='sk-xxxx',
        openai_api_base='https://api.deepseek.com', max_tokens=4096
    )
    chat_logger.error('connect_llm!!!!!!!!!!!!!!!!!!!!!!!')
    embeddings = ZhipuAIEmbeddings(
        model="embedding-3",
    )
    chat_logger.error('load embedding!!!!!!!!!!!!!!!!!!!!!!!')
    collection_name_list = ['Default_0', 'Default_1']
    chat_logger.error('read collection name list')

    vectorstore_list = [
        Milvus(
            connection_args=milvus_config, embedding_function=embeddings, collection_name=collection_name
        ) for collection_name in collection_name_list
        ]
    chat_logger.error('success connect Milvus!!!!!!!!!!!!!!!!!!!')

    retriever_list = [single_vectorstore.as_retriever().configurable_fields(
        search_kwargs=ConfigurableField(
            id="search_kwargs_0",
            name="doc's search_kwargs",
            description="doc's search_kwargs"
        )
    ) for single_vectorstore in vectorstore_list]
    chat_logger.error('build retriever success!!!!!!!')

    ensemble_retriever = EnsembleRetriever(
        retrievers=retriever_list, weights=[1 / len(retriever_list) for _ in range(len(retriever_list))]
    )
    chat_logger.error('ensemble success!!!!!!!!!!')

    template = """Answer the question based only on the following context:
            {context}
            Question: {question}
            """
    prompt = ChatPromptTemplate.from_template(template)
    chain = {"context": ensemble_retriever, 'question': RunnablePassthrough()} | prompt | llm

    questions = [m["content"] for m in messages if m["role"] == "user"][-1]
    chat_logger.error(f'questions:{questions}!!!!!!!!')

    try:
        mes = chain.invoke(str(questions), config=config)
    except Exception as e:
        chat_logger.error(str(e))

    chat_logger.error(f'mes content:-------------!!!!----!!!\n{str(mes)}')
    result = {"answer": str(mes).split("'")[1], "reference": [], "prompt": ""}

    chat_logger.error(f'llm_results!!!!!!!!!!!!!!!!{result}')
    yield result
    return result
```


---
