---
title: 'Solution to the bug that the filter does not take effect when searching multiple knowledge bases simultaneously in LangChain Part2'
date: 2024-09-19
permalink: /posts/2024/09/blog-post-15/
tags:
  - Bug analysis
  - Code-sharing
---

Yesterday, by carrying parameters in the HTTP request headers, I was able to resolve the issue of Langserve not passing parameters correctly. However, the downside of this method is that the headers are sent as plain text in HTTP requests, which means they can be seen or tampered with by others. While this might not involve sensitive core content, there are still potential security risks and concerns...

------
*The blog is translated by GPT4o, Read [CN version](https://yqwang96.github.io//cnposts/2024/09/blog-post-15/) of this post*


Today, I took a deeper dive into Langserve’s source code to understand how the framework handles parameter transmission. I implemented an alternative approach that allows parameters to be sent in the POST request's data, rather than in the headers, which enhances data security.

Specifically, after `add_routes`, Langserve wraps the chain in an `APIHandler` class. In this class, the `invoke` API is implemented. The `invoke` method forwards the user's request to the following method. In this method, parameters like `config_hash` and `server_config` cannot be specified by the user. If you want to modify them, you would need to rewrite parts of Langserve’s source code.

```python
async def invoke(
        self,
        request: Request,
        *,
        config_hash: str = "",
        server_config: Optional[RunnableConfig] = None,
    ) -> Response:
        """Invoke the runnable with the given input and config.

        Args:
            request: The request object.
            config_hash: A compressed representation of a config. Optionally
                         sent from the client side. This config must be validated.
            server_config: optional server configuration that will be merged
                with any other configuration. It's the last to be written, so
                it will override any other configuration.
        """
        # We do not use the InvokeRequest model here since configurable runnables
        # have dynamic schema -- so the validation below is a bit more involved.
        config, input_ = await self._get_config_and_input(
            request,
            config_hash,
            endpoint="invoke",
            server_config=server_config,
        )
        run_id = config["run_id"]

        event_aggregator = AsyncEventAggregatorCallback()
        _add_callbacks(config, [event_aggregator])

        ...
```

"In the above portion of the code, Langserve first integrates the input configuration using the `_get_config_and_input` function. In this function, the user’s input is parsed through a validation class to check if the input format meets the requirements.

```python
body = InvokeRequestShallowValidator.model_validate(body)
```

After validation, the input is parsed and format-checked. The key part of the parsing code is shown below. It reveals that the user-provided `config` can be of type `str`, `dict`, or `BaseModel`. Typically, configuration information sent via JSON is treated as a `dict`. However, Langserve cannot determine the type of the `config` at this point. The reason is that Langserve uses Pydantic for format validation (as indicated by the `model` variable in the code below). But the schema for the `model` is influenced by the `self._config_specs` variable in the `APIHandler`, and by default, this variable is an empty list, which prevents the schema from being generated."


```python
    config_dicts = []
    for config in client_sent_configs:
        if isinstance(config, str):
            config_dicts.append(model(**_config_from_hash(config)).model_dump())
        elif isinstance(config, BaseModel):
            config_dicts.append(config.model_dump())
        elif isinstance(config, Mapping):
            config_dicts.append(model(**config).model_dump())
        else:
            raise TypeError(f"Expected a string, dict or BaseModel got {type(config)}")
    config = merge_configs(*config_dicts)
```

"To address this, the user needs to override `self._config_specs` and add a custom schema for the configurable settings. The modified `WebChain` is as follows:"


```python
from typing import Optional, Any, Type, List, Dict
from langchain.retrievers import EnsembleRetriever
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnableConfig, Runnable, RunnableBinding, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.utils import Input, Output
from langchain_milvus import Milvus


class ConfigurableFieldSpec:
    def __init__(self, id: str, annotation: type, default: Any, name: str, description: str):
        self.id = id
        self.annotation = annotation
        self.default = default
        self.name = name
        self.description = description


class WebChain(Runnable):
    def __init__(self):
        super().__init__()
        self._config_specs = [
            ConfigurableFieldSpec(
                id="search_kwargs",
                annotation=Dict[str, Any],  # Expecting a dictionary
                default={},  # Default value as an empty dictionary
                name="Search Arguments",
                description="Parameters for search query."
            )
        ]

        self.milvus_config = {
            "host": "localhost",
            "port": 19530,
            "db_name": "default"
        }

        self.llm = llm

        self.embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L6-v2")

    @property
    def config_specs(self) -> List[ConfigurableFieldSpec]:
        """Override the config_specs to return the custom specs."""
        return self._config_specs

    def get_chain(self):
        collection_name_list = ['Default_0', 'Default_1']  # 从数据库中读取配置

        vectorstore_list = [
            Milvus(
                connection_args=self.milvus_config, embedding_function=self.embeddings, collection_name=collection_name
            ) for collection_name in collection_name_list
        ]

        retriever_list = [single_vectorstore.as_retriever().configurable_fields(
            search_kwargs=ConfigurableField(
                id="search_kwargs",
                name="doc's search_kwargs",
                description="doc's search_kwargs"
            )
        ) for single_vectorstore in vectorstore_list]

        # retriever_list = [single_vectorstore.as_retriever() for single_vectorstore in vectorstore_list]

        ensemble_retriever = EnsembleRetriever(
            retrievers=retriever_list, weights=[1 / len(retriever_list) for _ in range(len(retriever_list))]
        )

        template = """Answer the question based only on the following context:
        {context}
        Question: {question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = {"context": ensemble_retriever, 'question': RunnablePassthrough()} | prompt | self.llm
        return chain

    def invoke(
        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> Output:
        print(input)
        print(config)
        chain = self.get_chain()
        mes = chain.invoke(input, config=config)
        return mes
```

"Accordingly, the `requests` method for making the request is as follows:"

```python
import requests
from langchain.globals import set_verbose, set_debug

set_verbose(True)
set_debug(True)

query = "What impact does the education level of older workers have on the labor market?"

url = f"http://localhost:8002/rag/invoke"

json1 = {
    'input': query,
    'config': {
        "configurable": {
            "search_kwargs": {
                "expr": "collection_name == 'Default_1'"
            }
        }
    }
}

response = requests.post(url, json=json1)

# 打印返回的结果
if response.status_code == 200:
    print("Response:")
    print(response.json())
else:
    print(f"Failed with status code: {response.status_code}")
    print(response.text)
```

"Using this method, configuration information can be sent directly within the JSON payload, without the worry of it being exposed to or tampered with by third parties."

------
