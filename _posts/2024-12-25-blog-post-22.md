---
title: '[HierRL: Hierarchical Reinforcement Learning for Task Scheduling in Distributed Systems] Study Notes'
date: 2024-12-25
permalink: /posts/2024/12/blog-post-22/
tags:
  - Paper summary
---

Distributed systems are crucial for large-scale task processing. Among these, Ray, as an advanced distributed system, has gained significant attention. However, Ray's current two-layer scheduling mechanism (local scheduler and global scheduler) exhibits limitations in certain application scenarios, despite its flexibility and scalability. This article explores a novel approach—a task scheduling algorithm based on Hierarchical Reinforcement Learning (HierRL)—designed to optimize the scheduling performance in Ray.

*Read [CN version](https://yqwang96.github.io/cnposts/2024/12/blog-post-22/) of this post*

## Background

Ray is a system designed to support distributed execution for decision-making tasks, with its scheduling mechanism divided into local schedulers and a global scheduler. Tasks are initially submitted to the local scheduler. When a node is overloaded (e.g., the queue exceeds the threshold or resources are insufficient), tasks are forwarded to the global scheduler. However, this "bottom-up" scheduling strategy struggles with task randomness and uneven resource distribution. Improving the scheduling strategy can significantly enhance resource utilization and yield economic benefits.

Traditional scheduling optimization methods include heuristic algorithms and reinforcement learning algorithms. Heuristic algorithms often require manual design for specific task scenarios, performing well under certain conditions but lacking generalization. On the other hand, reinforcement learning is well-suited for dynamic and complex scheduling problems, as it does not rely on precise environmental modeling. HierRL leverages the advantages of reinforcement learning and integrates it into Ray's two-layer scheduling framework to deliver an efficient hierarchical scheduling method.

## HierRL Methodology

HierRL consists of two levels of agents:

1. **High-Level Agent (H-Agent):** Responsible for selecting tasks from the global scheduler and assigning them to appropriate local schedulers.
2. **Low-Level Agent (L-Agent):** Responsible for selecting tasks from the local task queue for execution.

Each agent is designed with distinct focus areas:

- **State Space:**
  - The H-Agent’s state includes the remaining resources, task queue length, and the number of tasks assigned by the global scheduler for each node.
  - The L-Agent’s state includes the resource requirements and submission time of the tasks to be selected.

- **Action Space:**
  - The H-Agent’s actions involve assigning tasks to specific nodes.
  - The L-Agent’s actions involve selecting tasks from the queue.

- **Reward Function:** The reward function encourages balanced resource allocation and reasonable queue length distribution, with the ultimate goal of minimizing task makespan.

To train these agents, the study employed a hierarchical policy learning method, using Q-value updates in reinforcement learning for efficient training.

## Experimental Evaluation

The experiments were conducted on the CloudSim platform, using a real-world task dataset released by the Alibaba Cluster Trace Program. This dataset contains task distributions over 12 hours from approximately 1,300 machines in a production environment.

### Experimental Results:

1. **Fixed Task Scenarios:**
   - Across scenarios with different task numbers (52, 171, 292) and node configurations (2 to 5 nodes), HierRL significantly reduced task completion time compared to Ray's original scheduling strategy and random scheduling:
     - At least a 10% reduction compared to Ray.
     - At least a 15% reduction compared to random scheduling.

2. **Random Task Scenarios:**
   - HierRL continued to outperform other methods in scenarios with randomly selected tasks for training and testing, achieving over a 5% reduction in task completion time on average.

3. **Impact of Node Numbers:**
   - As the number of nodes increased, the improvement achieved by HierRL diminished. This indicates that when nodes have abundant resources, the impact of scheduling optimization becomes less significant.

### Key Findings

The results demonstrate that HierRL excels across various task configurations and node scenarios, particularly in resource-constrained and complex task environments where its optimization effects are more pronounced.

## Conclusion and Future Work

By integrating hierarchical reinforcement learning strategies, HierRL effectively addresses the shortcomings of Ray’s two-layer scheduling mechanism. The high-level agent optimizes global resource utilization by learning task allocation strategies, while the low-level agent improves task queue management by learning task execution sequences. In the future, this method could be further integrated as a core module for Ray, providing stronger support for scheduling optimization in distributed systems.

Hierarchical reinforcement learning strategies like HierRL can also be extended to other distributed system domains, such as cloud resource management and edge computing task allocation, offering new approaches to intelligent distributed systems.

