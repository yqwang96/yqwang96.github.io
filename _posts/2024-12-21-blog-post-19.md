---
title: '[Performance and Cost-Efficient Spark Job  Scheduling Based on Deep Reinforcement  Learning in Cloud Computing Environments] Study Notes'
date: 2024-12-21
permalink: /posts/2024/12/blog-post-19/
tags:
  - Paper summary
---


Recently, I came across a fascinating research paper that tackles a critical issue in cloud computing: optimizing Apache Spark job scheduling for better performance and cost-efficiency. Here’s a detailed summary of the key insights I gained from this paper:

*Read [CN version](https://yqwang96.github.io/cnposts/2024/12/blog-post-19/) of this post*

---

### **Current Research Challenges**

1. **Limited Consideration of SLA Goals and Resource Allocation:**  
   While many existing studies focus on meeting specific Service Level Agreement (SLA) objectives, they often neglect the impact of executor creation and resource allocation on these goals.

2. **Homogeneous Cluster Assumptions:**  
   Most prior works assume clusters are homogeneous. However, real-world cloud environments are typically heterogeneous, offering diverse virtual machine (VM) configurations and pricing models that can be leveraged to reduce costs.

3. **Lack of Generalization in Scheduling Schemes:**  
   Current heuristic or performance-model-based scheduling approaches tend to focus on single scenarios and struggle to generalize across varying workload characteristics and multi-objective optimization needs.

---

### **Research Contributions**

This paper proposes a **Reinforcement Learning (RL)-based Spark Job Scheduling Model** with several notable contributions:

1. **Reward Function Design:**  
   A reward function is constructed to train Deep Reinforcement Learning (DRL) agents. The model optimizes cost efficiency, reduces the average execution time of cluster jobs, and meets resource constraints.

2. **Prototype Development:**  
   An RL-based model prototype was implemented in Python and integrated into the TensorFlow Agents (TF-Agents) framework.

3. **Deployment of Two DRL Agents:**  
   The authors developed two types of DRL agents—DQN and REINFORCE—which were trained and tested as scheduling agents in the TF-Agents framework.

---

### **Problem Modeling**

#### **Scenario Overview**

1. **VM Characteristics:**  
   The cloud environment consists of multiple types of VMs with varying resource capacities (e.g., CPU and memory).

2. **Job Submissions:**  
   Users can submit one or more jobs, specifying resource requirements (e.g., total executors, CPU cores, and memory). Job arrivals are random and diverse, with submissions possible at any time.

3. **Scheduler Task:**  
   The scheduler determines which VM to allocate executors to for each job.

#### **Optimization Goals**

1. **Cost Minimization:**  
   Reduce the total monetary cost of running the cluster.

2. **Execution Time Reduction:**  
   Lower the average job completion time.

#### **Key Constraints**

1. **Resource Capacity Constraints:**  
   The allocated CPU and memory resources for any VM cannot exceed its capacity.

2. **Uniqueness Constraint:**  
   Each executor can only be assigned to one VM.

#### **Objective Function**

- **Cost Calculation:**  
   The total cost of running the cluster is calculated as the product of VM usage duration and unit time pricing.
- **Average Job Completion Time:**  
   This is computed as the total execution time of all jobs divided by the number of jobs.
- **Goal:**  
   Minimize a weighted combination of total cluster costs and average job completion times.

---

### **Reinforcement Learning Framework**

#### **Simulation Execution Flow**
- The **scheduler agent** selects actions (e.g., allocating an executor to a VM).
- The **cluster manager** executes these actions.
- The **reward generator** evaluates actions based on predefined goals and provides feedback to the agent.

#### **State Space**
- Current resource availability and unit pricing for all VMs.
- Specifications of the current job, including job ID, executor CPU and memory requirements, and total executors needed.

#### **Action Space**
- The agent decides on one of the following:
  1. **Allocate executor to a specific VM.**  
  2. **Wait** if resources are insufficient, deferring executor creation until resources are freed.
- If there are N VMs in the cluster, the action space includes N+1 discrete actions:
  - **Action 0:** Wait and do not create any executors.
  - **Actions 1-N:** Allocate an executor to a specific VM.

#### **Reward Mechanism**
1. **Immediate Rewards:**
   - **Positive Reward:** Successfully allocating resources to an executor earns a fixed reward.
   - **Negative Reward:** Choosing to wait without creating an executor incurs a fixed penalty.
2. **Episodic Rewards:**
   - Agents receive additional rewards or penalties based on cost and completion time at the end of an episode.

---

### **Key Design Considerations**

1. **Algorithms and Methods:**  
   The DQN and REINFORCE algorithms were used to train agents. Although these methods are well-established, their application to this specific problem is innovative.

2. **Environmental Factors:**  
   - The execution time of jobs was influenced by executor placement, local resource contention, and workload distribution in public cloud settings.
   - Job execution profiles were simulated based on configurations derived from experimental cluster data.

---

### **Experimental Setup**

- **Job Resource Requirements:**  
  Randomly generated using a uniform distribution:
  - CPU cores: 1-6.
  - Memory: 1-10 GB.
  - Total executors: 1-8.

- **Job Types:**  
  Three distinct types of jobs were tested:
  1. **WordCount (CPU-intensive)**
  2. **PageRank (Network/IO-intensive)**
  3. **Sort (Memory-intensive)**

- **Job Arrival Patterns:**  
  Two scenarios were evaluated:
  - **Normal Mode:** 50 jobs arriving within one minute.
  - **Burst Mode:** 100 jobs arriving within ten minutes.

---

### **Baseline Algorithms**

The DRL-based scheduler was compared with several baseline algorithms:

1. **RR (Round Robin):**  
   The default Spark scheduling algorithm.

2. **RRC (Improved RR):**  
   A variant of RR that minimizes the number of VMs used.

3. **FF (First Fit):**  
   Allocates as many executors as possible to the first available VM to reduce costs.

4. **ILP (Integer Linear Programming):**  
   Uses an ILP solver to optimize executor placement.

5. **AEP (Adaptive Executor Placement):**  
   Dynamically switches between centralized and decentralized strategies based on job profiles.

---


