---
title: '[RLSK: A Job Scheduler for Federated Kubernetes Clusters based on Reinforcement Learning] Study Notes'
date: 2024-12-24
permalink: /posts/2024/12/blog-post-21/
tags:
  - Paper summary
---

In the modern cloud computing landscape, Kubernetes has become a cornerstone for managing containerized applications. However, efficiently scheduling workloads and balancing resource usage across federated clusters remains a significant challenge. This blog introduces RLSK (Reinforcement Learning Scheduler for Kubernetes), an innovative job scheduler designed to optimize resource allocation and scheduling in federated Kubernetes clusters using deep reinforcement learning.

*Read [CN version](https://yqwang96.github.io/cnposts/2024/12/blog-post-21/) of this post*

---

## **1. Background and Significance**

With the widespread adoption of cloud computing, data centers are running increasingly diverse workloads, with batch jobs still making up a significant proportion. Traditional resource management approaches often rely on manually designed and fine-tuned scheduling algorithms, which are not only time-consuming but also lack adaptability. Reinforcement learning (RL), as a dynamic optimization tool, has gained traction in recent years for solving scheduling problems, such as managing task dependencies and virtual machine resource allocation.

---

## **2. RLSK Design and Implementation**

The primary objective of RLSK is to enable adaptive, dynamic scheduling in multi-cluster environments. Its design consists of several core components:

### **2.1 System Architecture**
RLSK adopts a centralized scheduling model that interacts with master nodes in each Kubernetes cluster to collect resource utilization data and make scheduling decisions based on a pre-trained RL model. An auxiliary module is deployed on the master node of each Kubernetes cluster for data collection and job dispatching.

Key features of the architecture include:
- **Data Collection**: Collect CPU, memory, and other resource usage data from nodes via Kubernetes APIs.
- **Job Dispatching**: Relay scheduling decisions to Kubernetes’ internal scheduler.
- **Resource Management**: Unified management of pods and container resources within clusters.

### **2.2 Reinforcement Learning Model**
The RL model leverages Deep Q-Learning and is designed with the following elements:
- **State Representation**: Encodes resource usage and job characteristics as one-dimensional vectors, facilitating model training.
- **Action Definition**: Defines scheduling actions, such as assigning jobs to specific clusters or deferring jobs to prevent resource bottlenecks.
- **Reward Function**: Balances multiple objectives, including improving overall resource utilization, maintaining load balance across clusters, and equalizing resource utilization within each cluster.

### **2.3 Training Process**
During training, RLSK simulates job arrival sequences to generate state-action-reward samples, which are used to update its deep neural network (DNN). Using an ε-Greedy strategy, RLSK balances exploration and exploitation, gradually converging to an optimal scheduling strategy.

---

## **3. Performance Evaluation and Comparison**

To validate RLSK's effectiveness, the research team conducted simulation experiments and compared its performance with the following traditional scheduling algorithms:
- **Least Load**: Selects the cluster with the lowest load for each task.
- **Round Robin**: Distributes jobs evenly across clusters in sequence.
- **Random**: Assigns jobs to clusters randomly.
- **First Fit**: Allocates jobs to the first cluster that meets resource requirements.

### **Results**
1. **Resource Utilization**: RLSK significantly improves overall resource utilization and achieves better load balancing both across and within clusters.
2. **Reward Convergence**: While RLSK starts with lower rewards during early training stages, it quickly surpasses other algorithms and achieves consistently higher rewards.
3. **Makespan**: Although RLSK’s makespan is slightly higher than that of Least Load, its superior performance in resource balancing and utilization makes it a compelling choice for complex scheduling scenarios.

---

## **4. Conclusion and Future Work**

RLSK demonstrates the potential of deep reinforcement learning for intelligent scheduling in federated Kubernetes clusters. It successfully enhances resource utilization and scheduling efficiency by balancing resource usage across clusters and minimizing resource imbalances within them.

Future improvements to RLSK could include:
- Incorporating more sophisticated neural network models for improved accuracy.
- Handling dependent job scheduling across clusters.
- Scaling up to support larger and more complex cluster environments.

---

## **5. Research Value**

RLSK offers a fresh perspective on the resource scheduling challenges in multi-cluster environments, showcasing the powerful applications of reinforcement learning in cloud computing. By enabling more intelligent and adaptive scheduling, RLSK provides cloud service providers with a valuable tool to enhance resource management and deliver more stable and efficient services to users.