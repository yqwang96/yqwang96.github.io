---
title: 'Several Forms of Actor-critic Algorithm'
date: 2023-04-20
permalink: /posts/2023/04/blog-post-2/
tags:
  - simulation environment
  - reinforcement learning
---

In reinforcement learning, Actor-Critic is a classic algorithm. It integrates the features of policy-based algorithms and value-based algorithms. It can be used for both continuous action space problems and discrete action space problems. The algorithm contains two neural networks, the Actor network and the Critic network. During training, the Actor network interacts with the environment and is responsible for choosing actions based on the current observation state, while the Critic network evaluates the value of the current action of the agent based on the environmental state. Depending on the specific calculation methods used when updating the network, it has derived a variety of algorithms, such as the Advantage Actor-Critic (A2C) algorithm.

### Starting from the REINFORCE Algorithm

The REINFORCE algorithm is a classic reinforcement learning algorithm. Compared with value-based algorithms, it takes the action policy of the agent as the learning target, rather than learning the state value function of the environment. Also, the REINFORCE algorithm is based on Monte Carlo sampling, which means it must wait for each episode to completely end before updating again. Its update formula is as follows:

$$ \nabla_{\theta} J(\theta_t) = E_{\pi_{\theta}}[\nabla_{\theta} \log \pi_{\theta}(a_t|s_t)G_t]$$

Here, $\pi_{\theta}$ is the parameterized policy of parameter $\theta$, and $G_t$ is the sum of future rewards starting from time step $t$.

------
### Actor-Critic Algorithm

From the above formula, $G_t$ is equivalent to a *Critic*, which adjusts the update step length of the agent's action at time $t$ during the update. If $G_t$ is large, the gradient will update a longer distance in the direction of this action. At the same time, $G_t$ can also be generalized to other forms. There are several specific forms, such as:

1. $ \sum_{t=0}^{ \infty }r_t $ : Total return of the trajectory
2. $ \sum_{t'=t}^{ \infty }r_{t'} $ : Return after taking action
3. $ \sum_{t'=t}^{ \infty }r_{t'}-b(s_t) $ : Return minus the baseline
4. $ Q^{\pi}(s_t,a_t) $ : State-action function
5. $ A^{\pi}(s_t,a_t) $ : Advantage function
6. $ r_t+V^{\pi}(s_{t+1})-V^{\pi}(s_t) $ : TD-error

In the above several forms, 1-3 are calculated by applying the return of the trajectory, while 4-6 are the classic Actor-critic algorithm, and 5-6 are respectively the Advantage Actor-critic (A2C) algorithm and 1-step Advantage Actor-critic (A2C) algorithm. They apply the state-action value function or state value function to calculate. The characteristics of methods 4-6 are that they do not depend on Monte Carlo sampling and do not need to be updated after the episode ends.

Among them, $ A^{\pi}(s_t,a_t) $ can be calculated according to the one-step estimation or multi-step estimation of the advantage function, as shown in Formula 2. When using one-step estimation, it is the same as Form 6.

$$ A^{(n)}_t = r_t + \gamma r_{t+1} + \gamma ^2 r_{t+2} + ... + \gamma^n V(s_{t+n}) - V(s_t) $$

Using multi-step estimation can reduce the bias of the advantage function estimation, but at the same time, the variance of its estimation will increase. For this point, some scholars have proposed the [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) method to reduce the variance of the advantage function estimation. Also, it is worth noting that when using one-step estimation, the algorithm can update the network at each time step; while using N-step estimation, the algorithm needs to update once every N time steps, or update once every time after have experiences N steps in the Episode.

Moreover, according to my test results in the [CartPole environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/), as the step length of the advantage function estimation increases, the speed of the algorithm convergence will slow down, and the training effect has not improved significantly. Without introducing other tricks, the performance of the Advantage actor-critic algorithm in the CartPole environment is not very stable. The Reward curve often appears a cliff-like fall after a period of steady development. This may be because only a single actor and critic network are used in the algorithm, so there may be overestimation in the value. And this phenomenon may be improved after adopting the [DDPG algorithm](https://arxiv.org/abs/1509.02971). Of course, this might also be because my hyperparameter setting is not very good. :D

Reference links：
1. [https://zhuanlan.zhihu.com/p/29486661](https://zhuanlan.zhihu.com/p/29486661) （in chinese）

*Read [CN version](https://yqwang96.github.io/cnposts/2023/04/blog-post-2/) of this post*
------