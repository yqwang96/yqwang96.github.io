---
title: 'The difference between "Terminated" and "Truncated" in Gymnasium'
date: 2023-04-19
permalink: /posts/2023/07/blog-post-1/
tags:
  - cool posts
  - reinforcement learning
  - tage
---

Recently, while reviewing some classic reinforcement learning algorithms, I found that the commonly used library for algorithm verification, gym, has been updated and iterated, and a new simulation environment library, Gymnasium, has appeared. Compared with the early Gym library, the new version of the gym library and the Gymnasium library have some differences in the training process.

## env.reset
In the early version of the gym library, the `env.reset` function will reset the environment to the initial state and return the agent's observation state. However, in the updated Gymnasium library, the `env.reset` function will return the agent's initial observation state as well as a `debug_info` variable. Specifically, `debug_info` is a dictionary that stores supplementary information for the observation state. It is similar to the info variable in the return value of the `env.step()` function.

Based on my tests, in the Classic control environment, `debug_info` is often an empty dictionary. It can be ignored when used.

Reference link: [https://gymnasium.farama.org/api/env/#gymnasium.Env.reset](https://gymnasium.farama.org/api/env/#gymnasium.Env.reset)

## env.step
Like the `env.reset` function, the return value of the `env.step` function has also changed significantly. In the early version of the gym library, the return value of the `env.step` function includes four variables, which are:
- **`next_obs`**: Observation state at the next moment
- **`reward`**: The reward the agent gets in the current time window
- **`done`**: Whether the episode has ended
- **`info`**: Supplementary information

In the updated version, the return value of the `env.step` function includes five variables, which are:
- **`next_obs`**: Observation state at the next moment
- **`reward`**: The reward the agent gets in the current time window
- **`terminated`**: Indicates whether the agent has reached the termination state of the MDP constructed by the environment. This termination state could be the agent successfully completing the task, or it could be the agent reaching a state of complete failure. For example, in the [Cliff Walking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) task, whether the agent reaches the end point or the agent falls off the cliff, `terminated` will be `True`
- **`truncated`**: Indicates whether the MDP constructed by the environment has reached a truncated state. For example, in the classic [Mountain Car](https://gymnasium.farama.org/environments/classic_control/mountain_car/) environment, there is a possibility that the vehicle will never reach its destination. In such a case, we often set a truncation condition - if the vehicle has not reached its destination after N steps, we reset the environment and start the simulation of the next Episode. At this time `truncated` is `True`
- **`info `**: Supplementary information

Compared with the early version, the biggest difference is that the two termination states represented by `done` have been distinguished. This has a big impact on algorithms like Q-learning. In the Q-learning algorithm, the update of the network parameters is calculated by TD-error. Where

$$TD-error=y_{target}-Q(s,a;\\theta)$$

And the target value $y_{target}$ is calculated as

$$y_{target}=r + \\gamma * \\max_{a'} Q(s', a'; \\theta)$$

When the environment reaches the termination state, the last term $\\gamma * \\max_{a'} Q(s', a'; \\theta)$ in $y_{target}$ does not exist. However, when the environment is truncated, the last term should be calculated in $y_{target}$.

In the early version of the gym library, if only `done` is used, these two states cannot be distinguished, which will lead to incorrect value estimation by the agent. In the updated version, this point has been well remedied.

Reference links:
1. [https://www.kezhi.tech/5be4b154.html](https://www.kezhi.tech/5be4b154.html)
2. [https://www.gymlibrary.dev/api/core/#gym-env](https://www.gymlibrary.dev/api/core/#gym-env)
3. [https://github.com/openai/gym/issues/2510](https://github.com/openai/gym/issues/2510)

------
