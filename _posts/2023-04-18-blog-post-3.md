---
title: 'Some Remarks in the A3C Algorithm'
date: 2023-04-18
permalink: /posts/2023/04/blog-post-3/
tags:
  - simulation environment
  - reinforcement learning
  - A3C algorithm
---

Recently, I revisited the classic Asynchronous Advantage Actor-Critic (A3C) algorithm and perused the corresponding paper. Compared to the A2C algorithm, the A3C algorithm employs multiple asynchronous processes on a single CPU. This allows the algorithm to interact more quickly with multiple replicas of the current environment. While this improves sampling efficiency, it also reduces the correlation between training samples. According to the results in the original paper, this greatly enhances training speed and performance. As the saying goes, 'Review the old and know the new.' Reading the paper also reinforced some of my previous understandings. I am documenting them here for future reference.

## Update Frequency
The authors propose an asynchronous architecture in this paper. They applied it to the SARSA algorithm, 1-Step Q Learning algorithm, N-step Q Learning algorithm, and the Actor-critic algorithm. These algorithms have many variations, especially the Q Learning algorithm and the Actor-critic algorithm, such as the Dueling DQN algorithm. These variations can update network parameters at every training time. However, the authors did not adopt these methods in the original paper. Whether it's the 1-Step Q Learning algorithm, N-step Q Learning algorithm, or the Actor-critic algorithm, the authors accumulate gradients over multiple time steps or accumulate all the gradients of an episode before passing and updating the gradient. The authors took this approach to avoid frequent parameter updates, which might destabilize the learning process.

In particular, the N-step processing method in the N-step Q learning algorithm is different from that in the N-step DQN algorithm.
+ In the N-step DQN algorithm, a special Replay buffer is designed. Each row in the Replay buffer stores the starting state of the current trajectory and the environment state after a fixed N time steps, as well as the accumulated discounted rewards during these N time steps. After every N time steps, the network parameters are updated.
+ In the asynchronous N-step Q learning algorithm, the authors calculate the discounted rewards and gradients only after an episode ends. This process is similar to the calculation of $G_t$ in the REINFORCE algorithm. Specifically, for the last state of the time step, the gradient calculation only considers the current state's reward `r`. For the second to last state, its gradient calculation needs to consider both the current state's reward `r` and the reward `r` of the last state. And so on, when calculating the gradient of the starting state, it needs to consider the rewards `r` of all time steps, as shown in the algorithm in the red box below.

![Asynchronous NStepQ Learning Algorithm Pseudocode](https://github.com/yqwang96/yqwang96.github.io/blob/master/images/AsynchronousNStepQLearning.jpg?raw=true)

## Other Details
+ The authors derived the learning rate from random sampling in a log-normal distribution (ranging from $1e-4$ to $1e-2$).
+ It was observed in the paper that the effect of increasing the number of workers is more pronounced in the 1-step algorithm. The possible reason is that in the 1-step algorithm, the algorithm only observes the reward of the next time step, leading to a larger estimation bias. Increasing the number of workers allows the algorithm to explore more parts of the environment, thereby reducing the bias and hence the pronounced effect.
+ The authors also mentioned that applying entropy regularization in the objective function further enhances the stability of exploration.

In conclusion, compared to the Replay Buffer, using the Asynchronous framework reduces the required memory and computational resources. This allows it to achieve faster training speeds and efficiency on a single CPU than on a GPU. However, this doesn't mean that the Replay buffer is ineffective. The Replay buffer can enhance the utilization efficiency of samples. This remains effective in some environments where sampling is expensive. Integrating the Replay buffer with the Asynchronous framework is promising. Apart from the algorithms mentioned in the paper, the Asynchronous framework can also be applied to the PPO and TRPO algorithms.

**References**:
1. [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)

*Read [CN version](https://yqwang96.github.io/cnposts/2023/04/blog-post-3/) of this post*
------