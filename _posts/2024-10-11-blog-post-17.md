---
title: 'Dynamic scheduling of flexible bus services with hybrid requests and fairness: Heuristics-guided multi-agent reinforcement learning with imitation learning Study Notes'
date: 2024-10-11
permalink: /posts/2024/10/blog-post-17/
tags:
  - Paper summary
---

Even though I've already completed my PhD and my current work is not closely related to operations optimization or reinforcement learning, I am still unwilling to give up the knowledge I accumulated earlier. Therefore, I decided to start reading related papers again, particularly research on solving optimization problems using reinforcement learning.

------
*The blog is translated by GPT-4, Read [CN version](https://yqwang96.github.io/cnposts/2024/10/blog-post-17/) of this post*

Recently, I read a paper published in *Transportation Research Part B* (TRB) about flexible bus scheduling and learned some new things. To consolidate my understanding and share what I’ve learned, I wrote this study note.

The paper details are as follows:
Wu, W., Zhu, Y., & Liu, R. (2024). Dynamic scheduling of flexible bus services with hybrid requests and fairness: Heuristics-guided multi-agent reinforcement learning with imitation learning. *Transportation Research Part B: Methodological, 190*, 103069.

From the title of the paper, several key terms stand out. These terms provide insights into the focus of the paper:
+ **Flexible bus services**: Involves vehicle routing and planning to pick up and drop off passengers, the core of the paper.
+ **Dynamic scheduling**: Uses a rolling horizon framework. Since the paper adopts a rolling horizon framework, the authors refer to their research as dynamic scheduling.
+ **Hybrid requests**: Immediate and pre-booked requests. Pre-booked requests are given one day in advance, while immediate requests arise dynamically during vehicle operation. The authors also examine scenarios with and without demand prediction. In the predictive scenario, the immediate requests' origins and destinations (OD) and the number of passengers are predicted. In the non-predictive scenario, immediate requests are sampled randomly from historical data and assigned to the next period of the rolling horizon framework.
+ **Fairness**: Minimize the difference in waiting times for immediate requests. Specifically, the paper quantifies the degree of waiting time disparity for immediate requests using the **mean absolute deviation**.
+ **Reinforcement learning (RL)**: Uses RL algorithms for decision-making. The paper treats each vehicle as an agent, creating a multi-agent reinforcement learning problem where the RL algorithm decides the routing of the vehicles.
+ **Imitation learning**: Combines with heuristic algorithms to enhance the training performance of the RL algorithm. Imitation learning loss is computed from experience batches sampled from a local search buffer, and these experiences are used to train the RL algorithm by imitating better solutions produced by local search.
+ **Heuristic algorithms**: Guide the reinforcement learning search. For the trajectories stored in the RL experience replay buffer, a neighborhood search algorithm is applied to find improved trajectories, and the better trajectories are also stored to guide the RL training.

These research focuses also represent the key innovations of the paper. I will now delve into the details of the study.

+ **Abstract**
Flexible bus services (FBS), as a classic form of demand-responsive transport, offer door-to-door travel experiences, and their popularity is on the rise. However, this mode of transportation faces challenges like high dynamism and real-time demand. Previous studies have focused only on designing FBS for pre-booked requests, neglecting the potential market of immediate travel demand. By utilizing historical travel data, it is possible to optimize fleet usage and improve financial sustainability. In response, this paper proposes a multi-objective decision model that simultaneously optimizes vehicle routing, scheduling, waiting times, and passenger assignment decisions, with particular emphasis on fairness for immediate requests. The paper introduces a multi-agent reinforcement learning framework combined with local search, a rolling horizon framework, imitation learning, and a prediction algorithm. Numerical experiments demonstrate that the proposed method enhances training stability and solution quality compared to baseline methods.

+ **Research Contributions**
  + **Research Objective**: The study focuses on optimizing vehicle routes, schedules, stop control, and passenger allocation problems, a scenario that has not been explored in prior research. Notably, the paper considers hybrid requests and future demand prediction.
  + **Research Methods**:
    + Designing the state and action representations for reinforcement learning;
    + Designing action space pruning strategies to reduce infeasible action spaces;
    + Introducing imitation learning and heuristic search methods to guide the training of the RL algorithm;
  + **Experimental Design**: Conducts extensive numerical experiments and tests using real-world cases to validate the model and solution effectiveness.

+ **Problem Description**
  + **Service Area Modeling**: The FBS service area is modeled as a directed network, $$\mathcal{G}(\mathcal{V}, \mathcal{E})$$, where the set of nodes $$\mathcal{V} = I \cup J$$ includes physical stops (bus stops and depots). These nodes are connected by road segments (edges $$\mathcal{E}$$).
  + **Demand Modeling**:
    + **Pre-booked Requests**: These requests include the passenger’s pick-up point $$p_n \in I$$, drop-off point $$d_n$$, the number of passengers $$q_n$$, and the desired time window $$[t_n^s, t_n^e]$$. All such requests are collected one day in advance and are pre-planned.
    + **Immediate Requests**: Unlike pre-booked requests, immediate requests are dynamically added during the day of operation, and service providers need to adjust vehicle routes accordingly to accommodate these new demands.
  + **Optimization Objectives**:
    - **Minimize the total system cost**: Reduce operational costs through effective vehicle scheduling and time management.
    - **Minimize waiting time for immediate requests**: Reduce waiting times for immediate requests to improve passenger satisfaction and the attractiveness of the service.

+ **Research Assumptions**
  - **Multi-period optimization**: FBS is planned using a multi-period optimization method that covers the entire service duration. Apart from the last period, buses are not required to return to any specific depot.
  - **Bus departure and return**: Buses depart from depots at the start of each period and return to the depot at the end of each period.
  - **Multiple visits to stops**: Each stop can be visited multiple times by different vehicles during each period.
  - **Homogeneous vehicles**: All vehicles have the same capacity and performance.
  - **Rejectable immediate requests**: Operators have the right to reject immediate requests.

+ **Research Methodology**
    + **Rolling Horizon Framework**  
    The rolling horizon framework is used to break down the dynamic and complex scheduling problem into a series of interdependent subproblems. The entire planning horizon is divided into multiple periods $|G|$, each with a length of $H$. For each period $g$, the request set $N_g$ includes both pre-booked and immediate demands. In scenarios with demand prediction, immediate requests $N_i$ are predicted to appear during period $g$ and are considered during that period's planning phase. In the non-predictive scenario, these requests must be submitted before period $g$. Each period $g$ is further divided into $|T|$ time intervals, and each vehicle makes decisions at the start of each time interval.  
    *Although the paper uses a rolling horizon framework, it later models the problem using a Markov decision process (MDP), where the time windows are more finely divided, making the period-based division seem unnecessary.*

+ **Optimization Objectives**
  + **Minimization of Total System Costs, Including Operational and User Costs:**

    $$
    \min (C^f_g + C^u_g), \forall g \in G
    $$

    where $C^f_g$ represents the operational costs, calculated based on the bus mileage and waiting times. The formula is:

    $$
    C^f_g = \sum_{k=1}^{|K|} \sum_{t=1}^{|T|} \left[ \delta^f \cdot DIS(i_{g,t,k}, i_{g,t+1,k}) + \delta^r \cdot a^r_{g,t,k} \right]
    $$

    - $DIS(i_{g,t,k}, i_{g,t+1,k})$ represents the distance traveled by bus $k$ from location $i_{g,t,k}$ to $i_{g,t+1,k}$ during time interval $t$ in period $g$.
    - $\delta^f$ is the cost per unit of distance.
    - $\delta^r$ is the cost per unit of waiting time.
    - $a^r_{g,t,k}$ is the waiting time for bus $k$ during time interval $t$ in period $g$.

    $C^u_g$ represents the user costs, including penalties for late arrivals that exceed the last departure time and additional costs for rejecting requests. The formula is:

    $$
    C^u_g = \sum_{n \in N_g} \left[ (1 - y^n) \cdot \delta^s \cdot q_n \cdot \max(t^n_p - t^n_o, 0) + y^n \cdot \delta^n \right]
    $$

    - $y^n = 0$ means request $n$ is accepted, $y^n = 1$ means it is rejected.
    - $\delta^s$ is the penalty for being late per unit of time.
    - $\delta^n$ is the penalty for rejecting the request.
    - $q_n$ represents the number of passengers for request $n$ or the size of the request.
    - $t^n_p$ is the expected arrival time for request $n$, and $t^n_o$ is the actual arrival time.

  + **Fairness in Waiting Times**
    + **Calculation of Waiting Time**
      + When a request $n$ is accepted ($y^n = 0$), its waiting time $W_n$ is the expected service time $t^n_p$ minus the actual arrival time $t^n_o$.
      + If a request $n$ is rejected ($y^n = 1$), the waiting time is calculated as the difference between the actual arrival time $t^n_o$ and the time when the request is rejected $t^n_b$, if $t^n_b$ is greater than $t^n_o$; otherwise, the difference is zero.

    + **Fairness Metric Definition:**
      $$
      \xi_g = \frac{\sum_{n \in N_g} |W_n - \overline{W}_g|}{|N_g|}
      $$
      
      - $\overline{W}_g$ is the average waiting time for all requests during period $g$.
      - $\|W_n - \overline{W}_g\|$ represents the absolute difference between each request’s waiting time and the average waiting time.

    + **Optimization Objective: Minimizing $\xi_g$ to reduce the variance in waiting times and improve service fairness:**
      $$
      \min \xi_g = \frac{\sum_{n in N_g} |W_n - \overline{W}_g|}{|N_g|}, \forall g \in G
      $$

+ **Optimization Constraints**
  + **Vehicle Capacity Constraint**: $z_{g,t,k} \leq Z$ ensures that at any given time $t$, in any period $g$, the number of passengers $z_{g,t,k}$ on bus $k$ does not exceed its maximum capacity $Z$.
  + **Movement Constraints**:
  
    $$
    \sum_{g=1}^{|G|} \left[ \sum_{t=1}^{|T|} \frac{DIS(i_{g,t,k}, i_{g,t+1,k})}{vel} + \sum_{t=1}^{|T|} a^r_{g,t,k} \right] \leq |G| \cdot H
    $$

    Ensures that each bus $k$ completes all requests during the entire planning time $|G| \cdot H$ and returns to the depot.

  + **Delay Factor Constraint**:
  
    $$
    t^n_d + \frac{DIS(p^n, h^n)}{vel} \leq \alpha \cdot t^n_p
    $$

    Ensures the time efficiency of the FBS, so that the actual travel time from the passenger’s pick-up point $p^n$ to drop-off point $h^n$, plus the passenger’s waiting time $t^n_d$, does not exceed the expected time $t^n_p$ by more than $\alpha$ times.

+ **Multi-objective Reinforcement Learning Optimization**
  + **State**:
    + **Global State**: Consists of the vehicle status and the travel request status.
      + Vehicle status $S^K_{g,t}$ includes $t_{g,t,k}, z_{g,t,k}, t^{arl}_{g,t,k}$, representing the timestamp, location variable, and the arrival timestamp of vehicle $k$ after decision-making at time interval $t$. $t^{arl}_{g,t,k}$ also indicates whether the vehicle is occupied.
      + Request status $S^N_{g,t}$ represents all request statuses during time interval $t$ in period $g$. Specifically, the status of each request $n$, $S^N_{g,t,n}$, includes $v_n$, $t^p_n$, and $t^d_n$, which denote the vehicle currently assigned to request $n$, the request’s pick-up time, and the drop-off time.

    + **Local State**: Contains vehicle status and the timing information of the travel request.
      + Vehicle status includes the timestamp $t_{g,t,k}$, the location $z_{g,t,k}$, the arrival time $t^{arl}_{g,t,k}$, etc.
      + Travel request timing information includes the pick-up and drop-off times $\xi^p_{g,t,n}$ and $\xi^d_{g,t,n}$, indicating the urgency of each request.

  + **Action**: Divided into two dimensions:
    + $a^{\mathcal{M}}_{g,t,k}$: Represents the movement actions of the vehicle during time interval $t$, including visiting pick-up points, drop-off points, or returning to the depot.
    + $a^{\mathcal{H}}_{g,t,k}$: Represents the holding time (waiting time) before moving. This is considered only when the vehicle decides to pick up a request.
    
    + **Action Space Pruning**: Due to model assumptions and constraints, some actions may be infeasible in the original action space. The paper designs four action space pruning methods: constraints on visit sequence, delay constraints, movement constraints, and capacity constraints. The pruned action space is referred to as the “valid action space,” containing only feasible actions for each vehicle agent $k$ at specific times and conditions.

  + **Reward Function**: Cost reward $R^C$ and fairness reward $R^S$
    + **Cost Reward $R^C$**: Focuses on cost efficiency, minimizing operational costs and improving resource utilization.
    + **Fairness Reward $R^S$**: Ensures fairness in service, potentially related to serving requests or user groups more equitably.
    + **The total reward function** is given by the following formula:
  
      $$
      R(S_{g,t+1} \mid S_{g,t}) = \rho \cdot R^C(S_{g,t+1} \mid S_{g,t}) + (1 - \rho) \cdot R^S(S_{g,t+1} \mid S_{g,t})
      $$

      This formula ensures that both rewards are balanced based on the parameter $\rho$ after state transitions.


+ **Improvements to the $\epsilon$-greedy Strategy**
  In the classic $\epsilon$-greedy (e-greedy) strategy, time-related information has not been sufficiently considered. However, time information is crucial for the decision-making process, as it affects the availability of resources and the effectiveness of decisions. To this end, this paper introduces time-related information to enhance the efficiency of the exploration process. For each request $n$, the urgency levels $\xi^p_{g,t,n}$ and $\xi^d_{g,t,n}$ indicate the urgency of the request being picked up or delivered, respectively. The improved strategy selects the most urgent request $n^*$ as follows:
  
  $$
  n^* = \arg\min_n \left( \min \left\{ \xi^p_{g,t,n}, \xi^d_{g,t,n} \right\} \right)
  $$

  This formula ensures that among all selectable requests, the one with the smallest urgency, i.e., the one most in need of immediate handling, is chosen.
  Once the most urgent request has been identified, it is further checked whether this action of picking up $p_{n^*}$ or dropping off $d_{n^*}$ is a valid action, i.e., whether it falls within the valid action space. If the action related to $n^*$ is not valid, then vehicle $k$ will randomly select an action from the valid action space.

  The improved $\epsilon$-greedy strategy's action selection function is as follows:

  $$
  \Gamma(o_{g,t,k}) = 
  \begin{cases}
  p_{n^*}, & \text{if } \xi^p_{g,t,n} \neq \mathcal{M} \text{ and } \xi^p_{g,t,n} < \xi^d_{g,t,n} \text{ and } p_{n^*} \in \hat{A}_{g,t,k} \\
  d_{n^*}, & \text{if } \xi^d_{g,t,n} \neq \mathcal{M} \text{ and } \xi^d_{g,t,n} < \xi^p_{g,t,n} \text{ and } d_{n^*} \in \hat{A}_{g,t,k} \\
  \text{Random action from } \hat{A}_{g,t,k}, & \text{otherwise}
  \end{cases}
  $$

  If the most urgent request is valid, the vehicle prioritizes processing the most urgent action; otherwise, it randomly selects from the valid action space.

+ **Dual Exploration $\epsilon$-greedy Strategy**
  - Introducing two initial exploration rates, $\epsilon_1$ and $\epsilon_2$, ensures sufficient randomness in the early stages of training to explore new possibilities. Together $\epsilon_1 + \epsilon_2 \leq 1$, and these exploration rates gradually decay according to a certain mechanism as training progresses.
    - Probability $\epsilon_1(\tau)$: Performs a completely random action from the valid action space $\hat{A}_{g,t,k}$.
    - Probability $\epsilon_2(\tau)$: Chooses actions based on time-related information, i.e., selecting the most urgent request or the currently optimal action.
    - Probability $1 - \epsilon_1(\tau) - \epsilon_2(\tau)$: Chooses the current action estimated to provide the maximum return, i.e., the argmaxQ action.

    $$
    a^{\mathcal{M}}_{g,t,k} = 
    \begin{cases} 
    \text{Random action from } \hat{A}_{g,t,k}, & \text{with probability } \epsilon_1(\tau) \\
    \Gamma(o_{g,t,k}), & \text{with probability } \epsilon_2(\tau) \\
    \text{argmaxQ}(o_{g,t,k}, a_{g,t,k}), & \text{with probability } 1 - \epsilon_1(\tau) - \epsilon_2(\tau)
    \end{cases}
    $$

    where $argmaxQ(o_{g,t,k}, a_{g,t,k})$ represents the selection of the current greedy action that brings the maximum expected return.

+ **Neighborhood Search**
  Initial solutions generated by reinforcement learning are often not optimal, and local search attempts to find better solutions by searching in the neighborhood of the initial solution. The core of local search is designing effective local search operators, which perform moves such as swap, insert, or reverse, to explore possible improvements.  
  Neighborhood operation types: **Intra-loop move**, **Inter-loop move**, **Insert-rejected-request move**  
  Once an improved solution $x$ is obtained through neighborhood search operators, it is converted into experiences similar to those in reinforcement learning (RL) for further optimization.

+ **Loss Function**
  The formula for computing the Temporal Difference (TD) for RL is as follows:
  
  $$
  TD_{\text{RL}} = R_{g,t} + \gamma \sum_{k=1}^{|K|} \max_{a \in A_{g,t+1,k}} Q_k(o_{g,t+1,k}, a, \theta_g') - \sum_{k=1}^{|K|} Q_k(o_{g,t,k}, a_{g,t,k}, \theta_g)
  $$

  where $R_{g,t}$ is the reward at time $t$, $\gamma$ is the discount factor, and $Q_k$ is the action-value function for each agent. $\theta_g'$ is the parameter set of the target network used to calculate the maximum expected reward for the next state.

  In addition to this, imitation learning is introduced to enhance the training performance of reinforcement learning by using improved solutions from local search to optimize the loss function design.

+ **Training Data and Loss Function Evaluation**
  - Sample a batch of trajectories from the local search buffer, with trajectory data being:

    $$e^{LS} = (o^{LS}_{g,t}, A^{LS}_{g,t}, R^{LS}_{g,t}, o^{LS}_{g,t+1}, A^{LS}_{g,t+1})$$

  - Calculate the Temporal Difference error (TD-error) for each trajectory and evaluate the model's performance using the mean squared error:
  
    $$
    L^{IL} = \frac{1}{|a^{LS}|} \sum_{s=1}^{|a^{LS}|} \left[ \sum_{k=1}^{|K|} \left( \max_{a \in A^{LS}_{g,t,k}} Q_k(o^{LS}_{g,t,k}, a, \theta_g) + \delta(s^{LS}_{g,t,k}, a^{LS}_{g,t,k}) - Q_k(o^{LS}_{g,t,k}, a^{LS}_{g,t,k}, \theta_g) \right)^2 \right]
    $$

    where $\delta \left(s^{LS}_{g,t,k}, a^{LS}_{g,t,k} \right)$ is a margin function to avoid zero values in the loss function. When the predicted action is the maximum value action, this margin value is 0.8; otherwise, it is 0.

  - The total loss function is a combination of the TD-error loss $L^{TD}$ and the imitation learning loss $L^{IL}$:

    $$
    L = L^{TD} + L^{IL}
    $$

+ **Numerical Experiments**
  - **Network Layout**: The Sioux Falls network, which includes 24 sites, with one acting as a depot and the remaining 23 serving as transfer stations.
  - **Time and Demand**: For a single-period problem, demands are randomly generated, considering random distributions between starting and ending points at service stations.
  - **Multi-period Problems**: Demand generation follows a Poisson distribution, considering dynamic scenarios of demand changes from one station to another.
  - **Time Intervals**: Each period consists of 30 time intervals.
  - **Speed and Cost Coefficients**: Set parameters for unit transportation cost, late penalty cost, and other related cost factors.
  - **Reward Weights**: Set weights concerning cost and time for the rewards.
  - **Forecast Accuracy**: Describes the discrepancy between predicted and actual requests and sets the duration for predictions.

+ **Personal Summary**
This paper is quite comprehensive, heavy in workload, and contains some clever designs, such as the MDP state, exploration method design, and more, making it worthy of publication in TRB. However, it feels like the authors initially did not start with reinforcement learning but wanted to use a combination of optimization methods and rolling horizon frameworks to solve the problem through traditional mathematical optimization methods, which makes some parts seem a bit odd. But the use of heuristic methods combined with imitation learning to guide the training of reinforcement learning algorithms is still worth learning from.

Moreover, the paper contains a lot of unnecessary narrative, seemingly written to make the paper appear more complex and cater to the characteristics of the TRB journal.
