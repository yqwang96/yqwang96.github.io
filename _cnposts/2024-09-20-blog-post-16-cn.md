---
title: 'Ragflow对接Langchain和Milvus的踩坑日记'
date: 2024-09-22
permalink: /cnposts/2024/09/blog-post-16/
tags:
  - Bug analysis
  - Code-sharing
---

围绕着先前所提到的RAG知识库管理的开发需求，在上周五技术方案讨论会后，还是决定先尝试能否把已有的LangChain代码整合到ragflow的接口内，这样能够减少开发需求，避免重复造轮子。今天，在家里研究了一天，踩了一些坑，算是写了个demo，跑通了整体的流程。现在做一个记录，供有同样需求的朋友们参考；同时，也便于日后回顾。


在此对Ragflow和LangChain先不做过多介绍。感兴趣的朋友可以参考[RAGFlow Github仓库](https://github.com/infiniflow/ragflow)和[LangChain文档](https://python.langchain.com/v0.2/docs/introduction/)。说回现在的开发需求，目前已经实现了采用Langserve场景和不采用Langserve场景的知识库切换管理功能，接下来的任务是要将这部分功能整合到ragflow内。

在我研究基于langserve实现时，是在docker里拉取milvus镜像，然后创建容器进行测试的。而我的ragflow也是通过docker拉取镜像、创建容器部署的。所以，如果想要将Ragflow和Langchain进行整合，那么需要将这两个镜像组整合到一起。这是踩的第一个坑。

### Ragflow与milvus环境整合

milvus镜像组内包含milvus-standalone、etcd和minio三个镜像，ragflow镜像组则包含es、MySQL、minio、redis和ragflow-server五个镜像。所以，如果想要将这两个镜像组进行合并，那么需求解决minio重复的问题。一开始的解决方案是，将所有镜像写到同一个dockerfile内，然后只保存一个minio，确保其他奖项与minio之间的交互不存在问题。所以，用milvus内的minio代替了ragflow内的minio，成功拉取镜像，创建了容器。但是遇到的一个问题是<b>代码无法连接到milvus，无法做数据的向量化入库。即使是前端的向量知识库可视化软件也无法生效。milvus容器能够启动，但是一旦尝试连接的时候，容器将会自动关闭。查看日志也无法找到报错信息。</b>

起初，以为是minio或者etcd配置错误，milvus无法连接到这两个镜像，导致milvus自动关闭。但是三番五次调整，依然未遂，于是放弃这条路。转向分别拉取milvus镜像和ragflow镜像，停掉ragflow内的minio，只运行milvus内的minio即可。这种方法的问题是，要确保ragflow与milvus在同一个网络内。对此，在dockerfile内指定每个container的network为同一个，如下所示：

```md
... 
network
  - bridge
```

但是，因为milvus和ragflow是通过两个`docker-compose.yml`文件创建的，即使都将他们指定连接到同一个network。Docker依然会将他们分别连接到不同的网络内。解决办法是：

```
networks:
  my_shared_network:
    external: true
```

在创建第二个容器的时候，指定连接到同一个network内，并指定该网络是外部网络，不需要docker再次创建网络，便可以解决。随后，通过在ragflow-server容器内测试下述代码，发现可行。

```
ping 172.19.0.8 # milvus-standalone的IP
```

### ragflow聊天的接口调用流程

在想如何去测试ragflow聊天接口调用时，是通过以下流程：
1. 首先看ragflow这个容器的它自身的调用日志，发现在发送消息后，是通过一个completion这个接口去调用的，然后在pycharm里通过搜索去找到这个接口。
2. 在找到这个接口后，发现它是调用了那个dialog_service的Chat的接口.所以说下一步需求是在messenger里去拿到用户的问题，然后再通过chat去返回要求ragflow指定的格式就可以了。
3. 在后续的过程中，需要把代码去粘进去，然后通过pip install一些必须的库，然后另外在调试的时候，通过print的话，是没有办法打印出一些调试内容的。对此，是通过找了这个chat_logger这个变量，然后通过chat_logger.error记录一些信息，把这些error信息给他保存到一些无保存到TXT里，然后去看每一步执行的情况如何。

### 详细调试流程
在明确了上一部分之后呢，想要执行后续调试的话，我们需要首先把我们的文本去入库，然后我是通过在本地去访问暴露的端口，然后去做入库的。起初，我是用`huggingfaces/sentence-transfomer`进行入库的。然后对应的话，要在Retriever中呢，也采用这个embedding模型去提取出相应的对query做向量化，然后提取出相似文本。

但是问题在于，这个模型是通过本地加载的，虽然它模型的参数量可能不算很大，但是的话，它加载也需要时间。这个加载过程会导致ragflow后端在执行的时候卡住，对此，想到一个解决方法，就是通过调用现在一些成熟的API的方法。因为是走网络接口，不需要去部署再去执行这些，能够更快效率的去做embedding。

但是，后面执行的时候，我只修改了那个retriever中的embedding模型，没有在现代化入库的时候修改embedding模型，所以说导致有部分错误。对此的话，就是去重新针对新采用的embedding模型做向量化入库。嗯，在做向量化入库后，进行调试的过程中，通过在ragflow中不断的去打log的日志，去看每一部分的变量到底如何。


整体后面通过不断的修改response格式，然后使得Ragflow呢，把我们提取数据的文本给返回回去。对具体的针对知识库筛选，可以以我们最初的不套用langserve的With config的方法来进行筛选，

另外，如果说后期需要进行筛选的话，可以用ragflow的dialog中knowledge base的ID的信息从数据库里读出来，去构造那个config的json，最终传过去，实现筛选和回复。

整体流程如上。详细代码可参考下述内容（待补充）

```python
def chat(dialog, messages, stream=True, **kwargs):
    chat_logger.error('test chat!!!!!!!!!!!!!!!!!!!!!!!')
    import os
    import getpass
    from langchain_community.embeddings import ZhipuAIEmbeddings
    from typing import Optional, Any, Type, List, Dict
    from langchain.retrievers import EnsembleRetriever
    from langchain_core.runnables import ConfigurableField
    from langchain_openai import ChatOpenAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.runnables import RunnableConfig, Runnable, RunnableBinding, RunnablePassthrough
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables.utils import Input, Output
    from langchain_milvus import Milvus

    config = {
    'configurable': {
         "search_kwargs_0": {
             "expr": f"collection_name in ['Default_1']"
         }
     }
    }
    
    if not os.getenv("ZHIPUAI_API_KEY"):
        os.environ["ZHIPUAI_API_KEY"] = 'xxx.xxx'

    milvus_config = {
        "host": "172.19.0.8",
        "port": 19530,
        "db_name": "default"
    }

    llm = ChatOpenAI(
        model='deepseek-chat', openai_api_key='sk-xxxx',
        openai_api_base='https://api.deepseek.com', max_tokens=4096
    )
    chat_logger.error('connect_llm!!!!!!!!!!!!!!!!!!!!!!!')
    embeddings = ZhipuAIEmbeddings(
        model="embedding-3",
    )
    chat_logger.error('load embedding!!!!!!!!!!!!!!!!!!!!!!!')
    collection_name_list = ['Default_0', 'Default_1']
    chat_logger.error('read collection name list')

    vectorstore_list = [
        Milvus(
            connection_args=milvus_config, embedding_function=embeddings, collection_name=collection_name
        ) for collection_name in collection_name_list
        ]
    chat_logger.error('success connect Milvus!!!!!!!!!!!!!!!!!!!')

    retriever_list = [single_vectorstore.as_retriever().configurable_fields(
        search_kwargs=ConfigurableField(
            id="search_kwargs_0",
            name="doc's search_kwargs",
            description="doc's search_kwargs"
        )
    ) for single_vectorstore in vectorstore_list]
    chat_logger.error('build retriever success!!!!!!!')

    ensemble_retriever = EnsembleRetriever(
        retrievers=retriever_list, weights=[1 / len(retriever_list) for _ in range(len(retriever_list))]
    )
    chat_logger.error('ensemble success!!!!!!!!!!')

    template = """Answer the question based only on the following context:
            {context}
            Question: {question}
            """
    prompt = ChatPromptTemplate.from_template(template)
    chain = {"context": ensemble_retriever, 'question': RunnablePassthrough()} | prompt | llm

    questions = [m["content"] for m in messages if m["role"] == "user"][-1]
    chat_logger.error(f'questions:{questions}!!!!!!!!')

    try:
        mes = chain.invoke(str(questions), config=config)
    except Exception as e:
        chat_logger.error(str(e))

    chat_logger.error(f'mes content:-------------!!!!----!!!\n{str(mes)}')
    result = {"answer": str(mes).split("'")[1], "reference": [], "prompt": ""}

    chat_logger.error(f'llm_results!!!!!!!!!!!!!!!!{result}')
    yield result
    return result
```
---
