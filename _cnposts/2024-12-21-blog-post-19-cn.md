---
title: 'Performance and Cost-Efficient Spark Job  Scheduling Based on Deep Reinforcement  Learning in Cloud Computing Environments 学习笔记'
date: 2024-12-21
permalink: /cnposts/2024/12/blog-post-19/
tags:
  - Paper summary
---

最近我阅读了一些大数据集群调度的文章。目前，基于强化学习的调度方法是该领域的研究主流。今天分享的这篇文章是较早利用强化学习进行调度，且具有一定代表性。为此，分享这篇文章于此。


## 研究现状

- 许多现有研究集中于特定的 SLA 目标，但忽略了 executor 创建及资源分配对 SLA 目标的影响。
- 假设集群是**同质**的，而实际云环境中集群通常是**异质**的，不同虚拟机的定价模型可以被利用以降低成本。
- 现有的基于启发式或性能模型的调度方案通常专注于单一场景，缺乏对多目标优化及工作负载特性的泛化能力。

---

## 研究贡献

- 提出基于RL的Spark作业调度模型，构建奖励函数，用于训练 DRL 智能体以满足资源约束、优化成本效率并减少集群作业平均执行时间。
- 开发RL模型的原型。使用 Python 环境实现并集成至 TF-Agents 框架。
- 实现两种DRL智能体，DQN 和 REINFORCE。将其训练为基于 TF-Agents 框架的调度代理。

---

## 问题建模

问题：

- 多种类型虚拟机，每种类型的资源容量（如 CPU 和内存）不同。
- 用户可以提交一个或多个作业，作业中需指定资源需求（如 executor 总数及其 CPU 和内存大小）。作业的到达时间具有随机性，且类型多样，可以在任何时间提交。
- 调度器需要决定在哪些虚拟机上为当前作业创建 executors。

目标：

- 成本优化：减少集群运行的总货币成本。
- 时间优化：降低作业的完成时间。

建模：

- 资源：假设有N个虚拟机部署Spark集群，每个虚拟机实例的资源容量不同。
- 作业：共有M个需要调度的作业。作业通过调度器创建executors，分配到一个或多个虚拟机上。必须满足虚拟机的资源容量约束。
- 资源需求维度：CPU核心数和内存

约束条件：

- 资源容量约束：对于任意虚拟机，其所分配的内存和CPU不能超过VM的容量；
- 唯一性约束：每个executor只能分配到一个虚拟机；

目标函数：

- 成本计算：集群运行总成本为每个虚拟机使用时长乘上其单位时长费用；
- 平均作业完成时间：所有作业总时长/作业数；
- 目标：最小化集群运行成本与平均作业完成时间的加权和

---

## 强化学习建模

### **仿真环境执行流程**

- 调度器智能体选择动作（即为作业创建executor的VM）
- 集群管理器在集群中执行该动作；
- 奖励生成器根据预定目标评估行动，并为智能体生成奖励；

### 状态空间

- 所有虚拟机当前的资源可用性、单位价格；
- 当前作业规格：作业ID标识，作业executor的CPU和内存需求，该作业需要创建的总executor数量；

### **动作空间**

- 为当前作业的一个 executor 选择要创建的虚拟机（VM）。
- 如果集群中没有足够的资源分配给当前作业的一个或全部 executors，代理可以选择不执行调度（等待之前的作业完成释放资源）。
- 例如：集群中有N个虚拟机时，动作空间包括N+1个离散动作；
    - 动作0：智能体选择等待，不创建任何executor；
    - 动作1至N：智能体选择在指定虚拟机上创建executor；

### **奖励机制**

- 即时奖励：执行完每个动作后将会获得
    - 正奖励：每成功为当前作业的 executor 分配资源，环境给予一个固定的正奖励。
    - 负奖励：如果代理选择等待（Action 0），没有创建 executor，则环境给予一个固定的负奖励。
    
    在训练初期：
    
    - 初始训练周期中，固定奖励帮助代理逐步掌握资源约束条件；
    - 如果代理未能满足作业或虚拟机的约束条件：
        - 当前周期会提前终止。
        - 代理会收到一个大的负周期性奖励（episodic reward），以惩罚失败的调度行为。（没有通过动作mask模块去禁止）；
- 周期奖励：执行完整个episode将会获得
    - 周期性成本奖励：该episode所消耗的资源成本；
    - 周期性平均完成时间奖励：该episode所需要的时间成本；

---

## 强化学习算法与环境设计

- 见论文5.1章节与5.2章节；
- DQN与REINFORCED方法，所研究方法上没有创新；

值得注意的是：

- 环境考虑了以下因素对作业执行时间的影响：
    - 不同虚拟机上的 executor 放置引起的本地性和资源争用。
    - 公有云中任务执行的争用情况。
- 其中，作业执行时间的数据来自于：模拟环境从运行实际作业的实验性集群中收集作业配置文件。

---

## 实验设置

作业需求生成使用**均匀分布**随机生成，范围包括：

- CPU核心数：1-6；
- 内存：1-10；
- 总executor数：1-8；

作业类型包括：

- WordCount：CPU密集型；
- PageRank：网络或IO密集型；
- Sort：内存密集型；

作业到达时间：

- 正常模式：1分钟内到达50个作业；
- 突发模式：10分钟内到达100个作业；

---

## 基线算法

- RR：Spark默认调度算法；
- RRC：RR的改进版本，目标是最小化使用的虚拟机数量。
- FF：尽可能多地将执行器分配到第一个可用虚拟机上，以减少成本。
- ILP：使用混合整数线性规划（Mixed ILP Solver）求解当前作业的执行器放置问题。
- AEP：基于作业配置文件信息，在中心化和分散化的执行器放置策略之间动态切换。