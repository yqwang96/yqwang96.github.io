---
title: 'HierRL: Hierarchical Reinforcement Learning for Task Scheduling in Distributed Systems 学习笔记'
date: 2024-12-25
permalink: /cnposts/2024/12/blog-post-22/
tags:
  - Paper summary
---

分布式系统在大规模任务处理中的重要性不言而喻。在这个领域，Ray作为一种先进的分布式系统，受到了广泛的关注。然而，Ray目前采用的两层任务调度机制（本地调度器和全局调度器）虽然具有一定的灵活性和可扩展性，但其性能在某些应用场景中表现较为有限。这篇文章探讨了一种新方法：基于层次强化学习（Hierarchical Reinforcement Learning，简称HierRL）的任务调度算法，旨在优化Ray系统的调度性能。

**[详细总结](https://yqwang96.github.io/files/blog_pdf/summary_HierRL_Hierarchical_Reinforcement_Learning_for_Task_Scheduling_in_Distributed_Systems.pdf)**

## 背景介绍

Ray是一个为决策任务提供分布式运行支持的系统，其调度机制包括本地调度器和全局调度器两部分。任务会优先被提交到本地调度器，当节点过载时（例如队列超出阈值或资源不足），任务会被转移到全局调度器处理。然而，这种"自底向上"的调度策略在处理任务随机性和资源分布不均的情况下存在显著不足。提高调度策略的性能，可以显著提升资源利用率，带来经济效益。

传统的调度优化方法包括启发式算法和强化学习算法。启发式算法通常需要对具体任务场景进行人工设计，虽然在特定条件下表现优秀，但缺乏泛化性；而强化学习方法由于无需精确的环境建模，适合处理动态且复杂的调度问题。本文提出的HierRL正是利用强化学习的优势，结合Ray的两层调度框架，设计了一种高效的层次化调度方法。

## HierRL 方法详解

HierRL由两个层次的智能体（Agent）组成：

1. **高层智能体（H-Agent）**：负责从全局调度器中选择任务，并将其分配到合适的本地调度器。
2. **低层智能体（L-Agent）**：负责从本地任务队列中选择任务并执行。

这两个智能体的设计各有侧重：

- **状态空间**：
  - 高层智能体的状态包含每个节点的剩余资源、任务队列长度以及全局调度器分配的任务数。
  - 低层智能体的状态则包括待选任务的资源需求和提交时间。

- **动作空间**：
  - 高层智能体的动作是将任务分配给具体节点。
  - 低层智能体的动作是从任务队列中选择任务。

- **奖励函数**：奖励函数鼓励资源分配均衡和队列长度合理分布，同时以减少任务完成时间（makespan）为目标。

为了训练这两个智能体，本文采用了分层策略学习方法，通过强化学习中的Q值更新机制实现高效训练。

## 实验评估

实验在CloudSim平台上进行，使用阿里巴巴发布的真实任务数据集Alibaba Cluster Trace Program进行验证。该数据集包含了真实生产环境中约1300台机器在12小时内的任务分布情况。

### 实验结果：

1. **固定任务场景**：
   - 在不同任务数量（52、171、292）和节点配置（2节点到5节点）的场景下，与Ray原有调度策略和随机调度策略相比，HierRL能显著减少任务完成时间：
     - 相比Ray减少了至少10%。
     - 相比随机调度减少了至少15%。

2. **随机任务场景**：
   - 随机选择任务进行训练和测试，HierRL依然表现出了优于其他方法的性能，任务完成时间平均减少5%以上。

3. **节点数量的影响**：
   - 随着节点数量的增加，调度优化的提升效果有所降低。这表明在节点资源充裕时，调度策略对性能的影响相对减弱。

### 关键结论

实验结果表明，HierRL在多种任务配置和节点场景下均表现优异，尤其是在资源受限和任务复杂的情况下，其优化效果更加显著。

## 总结与展望

HierRL通过结合层次化的强化学习策略，有效地解决了Ray系统中两层调度策略的不足。高层通过学习任务分配策略优化全局资源利用，低层通过学习任务执行顺序优化任务队列管理。未来，该方法可以作为Ray系统的核心模块进一步集成，为分布式系统的调度优化提供更强大的支持。在实际应用中，类似的层次化强化学习策略也可以推广到更多的分布式系统领域，例如云计算资源管理和边缘计算任务分配，为智能化的分布式系统提供新的解决思路。

