---
title: 'Dynamic scheduling of flexible bus services with hybrid requests and fairness: Heuristics-guided multi-agent reinforcement learning with imitation learning 学习笔记'
date: 2024-10-11
permalink: /cnposts/2024/10/blog-post-17/
tags:
  - Paper summary
  - Code-sharing
---

虽然已经博士毕业了，并且现在工作的内容与运筹优化、强化学习关系不大，但我不愿意放弃之前积累的知识。因此，我重新开始阅读相关论文，特别是关于强化学习求解优化问题的研究。

最近，我阅读了一篇发表在TRB上的关于灵活公交调度的论文，学到了一些新知识。为了巩固对该论文的理解，也为了分享这些知识，我写下了这篇学习笔记。

论文的信息如下：
Wu, W., Zhu, Y., & Liu, R. (2024). Dynamic scheduling of flexible bus services with hybrid requests and fairness: Heuristics-guided multi-agent reinforcement learning with imitation learning. Transportation Research Part B: Methodological, 190, 103069.

从论文题目来看，这篇文章有着以下几个关键词。从这些关键词延伸，能够窥得这篇文章的研究重点。
+ **灵活公交**：涉及车辆行驶路径规划，研究如何接送乘客，是文章的核心内容。
+ **动态调度**：滚动时域框架。因为这篇文章使用了滚动时域框架，所以作者将他们的研究称作为动态调度；
+ **混合需求**：即时需求+预约需求。预约需求需要提前一天给出，即时需求将在车辆行驶时随时出现；作者还把场景分为了使用预测算法与不使用预测算法两种。在使用预测算法的场景中，即时需求的OD与订单乘客数将由预测算法得到；在不使用预测算法的场景中，即时需求将从历史需求中随机采样得到，并被分派到滚动时域框架的下一个period中；
+ **公平性**：即时需求的等待时间差别尽可能小。具体地说，这篇文章是采用了<b>平均绝对差</b>量化即时需求的等待时间离散程度。
+ **强化学习**：强化学习算法做决策。文章将每个车辆看作为一个智能体，整体上是一个多智能体强化学习问题，以强化学习算法去决策车辆的行驶路径。
+ **模仿学习**：与启发式算法结合增强强化学习算法训练性能。从局部搜索缓冲区抽样的经验批次计算模仿学习损失，这些经验被用来训练RL算法，以模仿局部搜索产生的更优解。
+ **启发式算法**：指导强化学习搜索。对于强化学习经验回放区内的轨迹，采用邻域搜索算法对其搜索得到一个更好的轨迹，并将更好的轨迹也给储存起来，在强化学习算法训练的时候进行指导。

这些研究重点也是文章的主要创新点所在。接下来，将详细叙述这篇文章的细节内容。

+ **摘要**
弹性公交（Flexible bus services，FBS）作为经典的需求响应式交通方式，能够为人们提供门到门的出行体验，受欢迎程度正在逐渐提高。但是，这种交通方式同时也面临着高动态性、需求即时性等挑战。以往的研究只关注服务预约需求的FBS设计，忽略了即时出行需求这一潜在市场。并且，利用历史出行数据，能够优化FBS车队的载客状况，并提高财务可持续性。对此，这篇文章提出了一个多目标决策模型来同时优化FBS的车辆路线、时刻表、等待时间与乘客分配决策，并特别强调了针对即时出行需求的公平性问题。这篇文章提出了一个结合局部搜索、滚动时域框架、模仿学习、预测算法的多智能体强化学习框架。通过数值实验验证发现，所提出的方法相比基准方法能够在训练稳定性和求解方案的质量上有进一步的提升。
+ **研究贡献**
  + **研究对象**：研究优化车辆路线、时刻表、停靠控制和乘客分配问题。这个优化场景是先前其他研究没有考虑过的。特别是，这篇文章考虑了混合请求和未来预测需求。
  + **研究方法**：
    + 设计强化学习的状态和动作表示；
    + 设计动作空间的修剪策略，减少不可行动作空间；
    + 引入仿真学习和启发式搜索方法，引导强化学习的训练；
  + **实验对象**：设计广泛的数值实验和基于真实世界的案例，验证了模型和解决方案的效果；
+ **问题描述**
  + **服务区域建模**：FBS的服务区域被建模为一个有向网络 $$\mathcal{G}(\mathcal{V}, \mathcal{E})$$，其中节点集合 $$\mathcal{V} = I \cup J$$ 包括物理停靠站（公交站和车库）。这些节点通过网络中的路段（边 $$\mathcal{E}$$）相连。
  + **需求建模**：
    + **预约请求**：这类请求定义为包含乘客的上车点 $$p_n \in I$$，下车点 $$d_n$$，乘客数量 $$q_n$$，以及期望的时间窗 $$[t_n^s, t_n^e]$$。所有这些请求都在服务开始前一天被收集，并被预先纳入运营计划。
    + **即时请求**：不同于预约请求，即时请求在运营当天动态地加入到最新的规划周期中，服务提供者需要相应调整车辆行驶计划以包含这些新增的需求。
  + **优化目标**：
    - **系统总成本最小化**：通过有效的车辆调度和时间管理来减少整个系统的运营成本。
    - **即时请求等待时间最小化**：尽量减少即时请求的等待时间，以提升乘客满意度和服务吸引力。
- **研究假设**
  - **多周期优化**：利用多周期优化方法来规划FBS，覆盖整个服务时间范围。除最后一个周期外，公交车不需要返回任何特定的车库。
  - **公交出发返回**：在每个周期内，公交车从车库出发，并在周期结束时返回车库。
  - **多次停靠点访问**：每个停靠点可能在每个周期内被多辆车辆多次访问。
  - **车辆同质性**：所有车辆具有相同的容量和性能。
  - **即时请求可拒绝**：运营方有权拒绝接受即时请求。
+ **研究方法**
    + **滚动时域框架**
    滚动时域框架用于将动态复杂的调度问题分解为一系列相互依赖的子问题。整个规划时域被划分为多个周期 $|G|$，每个周期长度为 $H$。每个周期 $g$ 对应的请求集 $N_g$ 包括预约需求和即时需求。在采用需求预测的操作中，即时请求 $N_i$ 预计会在周期 $g$ 内提交，并在该周期的规划阶段被纳入考虑。在没有需求预测的操作中，这些请求需要在周期 $g$ 之前提交。每个周期 $g$ 被进一步划分为 $|T|$ 个时间间隔，每个车辆在每个时间间隔开始时做出一次决策。
    
    *虽然文章采用了滚动时域框架，但其实后续使用的马尔科夫决策过程（MDP）进行建模，时间窗划分更为精细，因此，这种粗略的周期划分似乎并不必要。*

+ **优化目标**
  + **总系统成本，包括运营成本和用户成本：**
    $$\min (C^f_g + C^u_g), \forall g \in G$$
    其中，$C^f_g$ 为运营成本，可通过巴士的行驶里程和等待时间来计算。具体公式为：

    $$C^f_g = \sum_{k=1}^{|K|} \sum_{t=1}^{|T|} \left[ \delta^f \cdot DIS(i_{g,t,k}, i_{g,t+1,k}) + \delta^r \cdot a^r_{g,t,k} \right]$$

      - $DIS(i_{g,t,k}, i_{g,t+1,k})$ 表示周期 $g$ 中时间间隔 $t$ 内，巴士 $k$ 从位置 $i_{g,t,k}$ 行驶到 $i_{g,t+1,k}$ 的距离。
      - $\delta^f$ 是单位距离的成本。
      - $\delta^r$ 是巴士等待的单位时间成本。
      - $a^r_{g,t,k}$ 是巴士 $k$ 在周期 $g$ 的时间间隔 $t$ 的等待时间。
    $C^u_g$ 为用户成本，包括因晚到而超过最后离开时间的罚款以及拒绝请求的额外成本。其计算公式为：

    $$C^u_g = \sum_{n \in N_g} \left[ (1 - y^n) \cdot \delta^s \cdot q_n \cdot \max(t^n_p - t^n_o, 0) + y^n \cdot \delta^n \right]$$

      - $y^n = 0$ 表示请求 $n$ 被接受，$y^n = 1$ 表示被拒绝。
      - $\delta^s$ 是迟到每单位时间的罚款。
      - $\delta^n$ 是拒绝请求的罚款。
      - $q_n$ 是请求 $n$ 的乘客数量或请求的规模。
      - $t^n_p$ 是请求 $n$ 的期望到达时间，$t^n_o$ 是实际到达时间。

  + **等待时间的公平性**
    + **等待时间的计算**
      + 当请求 $n$ 被接受时 ($y^n = 0$)，其等待时间 $W_n$ 是期望的服务时间 $t^n_p$ 减去实际到达时间 $t^n_o$。
      - 如果请求 $n$ 被拒绝 ($y^n = 1$)，等待时间被计算为从实际到达时间 $t^n_o$ 到请求被拒绝的时间 $t^n_b$，如果 $t^n_b$ 大于 $t^n_o$，则取它们之间的差值，否则为零。
    - **公平性指标的定义：**

      $$\xi_g = \frac{\sum_{n \in N_g} |W_n - \overline{W}_g|}{|N_g|}$$

      - $\overline{W}_g$ 是周期 $g$ 内所有请求等待时间的平均值。
      - $\|W_n - \overline{W}_g\|$ 表示每个请求的等待时间与平均等待时间的绝对差值。
    - **优化目标：最小化 $\xi_g$，以减少等待时间的差异，从而提高服务的公平性：**

      $$\min \xi_g = \frac{\sum_{n in N_g} |W_n - \overline{W}_g|}{|N_g|}, \forall g \in G$$

+ **优化约束**
  + **车辆容量约束：** $z_{g,t,k} \leq Z$ 表示在任何给定的时间 $t$，在任何周期 $g$，任何巴士 $k$ 上的乘客数 $z_{g,t,k}$ 不得超过其最大容量 $Z$。
  + **运动约束：**

    $$\sum_{g=1}^{|G|} \left[ \sum_{t=1}^{|T|} \frac{DIS(i_{g,t,k}, i_{g,t+1,k})}{vel} + \sum_{t=1}^{|T|} a^r_{g,t,k} \right] \leq |G| \cdot H$$

    确保每辆公交 $k$ 在整个规划时间内完成对所有请求的服务并返回车库。
  + **延迟系数约束：**

    $$t^n_d + \frac{DIS(p^n, h^n)}{vel} \leq \alpha \cdot t^n_p$$

    确保FBS的时间效率，使得从乘客的上车点 $p^n$ 到下车点 $h^n$ 的实际旅行时间加上乘客的等待时间 $t^n_d$ 不会超过其期望时间 $t^n_p$ 的 $\alpha$ 倍。

+ **强化学习多目标优化**
  + **状态**：
    + **全局状态**：车辆状态与出行请求状态。
      + 车辆状态 $S^K_{g,t}$ 包括 $$t_{g,t,k}$$、 $$z_{g,t,k}$$、 $$t^{arl}_{g,t,k}$$ ，其含义分别为：时间戳、位置变量、车辆 $k$ 在决策完成后返回到时间间隔 $t$ 的到达时间戳。 $$t^{arl}_{g,t,k}$$ 相当于表示车辆是否已经被占用。
      + 请求状态 $S^N_{g,t}$ 是时间间隔 $t$ 在周期 $g$ 时的所有请求状态的集合。具体地说，每个请求 $n$ 的状态 $S^N_{g,t,n}$ 包括 $v_n$、$t^p_n$、$t^d_n$，其含义分别为：请求 $n$ 当前被分配的车辆的索引、请求的接取时间、请求的下车时间；
    + **局部状态**：车辆状态与出行请求的时间信息
      + 车辆状态：包括时间戳 $t_{g,t,k}$ 、位置 $$z_{g,t,k}$$、到达时间 $$t^{arl}_{g,t,k}$$ 等
      + 出行请求的时间信息：包括乘客请求的接取和下车时间 $\xi^p_{g,t,n}$ 和 $\xi^d_{g,t,n}$，这些信息表明每个请求的紧迫性，即需要被接取或送达的程度。
  + **动作**：分为两个维度；
    + $a^{\mathcal{M}}_{g,t,k}$：表示车辆在时间间隔 $t$ 的移动动作，包含了如访问接客点、放客点或返回车库的行为。
    + $a^{\mathcal{H}}_{g,t,k}$：表示车辆在移动之前的等待时间（holding time），此等待时间仅在车辆决定接取一个请求时才被考虑。
    + **动作空间剪枝**：在原始动作空间中，由于模型假设和约束条件的存在，某些动作可能并不可行。这篇文章设计了四种动作空间剪枝的方法：如访问顺序的约束、迟到约束、移动约束、容量约束；修剪后的动作空间被称为“有效动作空间”（valid action space），仅包含对于每个特定车辆智能体 $k$ 在特定时间和条件下可行的动作。
  + **奖励函数**：成本奖励 $R^C$ 与公平奖励 $R^S$
    + **成本奖励 $R^C$**：关注成本效率，即如何最小化运营成本或提高资源利用效率。
    + **公平奖励 $R^S$**：确保服务的公平性，可能涉及平等地服务于不同的请求或用户群体。
    + **总奖励函数** 由以下公式给出：

      $$
      R(S_{g,t+1} \mid S_{g,t}) = \rho \cdot R^C(S_{g,t+1} \mid S_{g,t}) + (1 - \rho) \cdot R^S(S_{g,t+1} \mid S_{g,t})
      $$

      这个公式确保了在状态转换后，两种奖励的权衡能够通过 $\rho$ 的调整得到适当的反映。

  + **epsilon-greedy改进**
    在经典 $\epsilon$-贪心（e-greedy）策略中，时间相关的信息没有被充分考虑。然而，对于决策过程来说，时间信息是一个重要的因素，因为它影响资源的可用性和决策的有效性。为此，这篇文章引入请求的时间相关信息，以提高探索过程的效率。
    对于每个请求 $n$，紧急程度 $\xi^p_{g,t,n}$ 和 $\xi^d_{g,t,n}$ 分别表示请求被接取或被送达的紧迫程度。在改进后的策略中，车辆 $k$ 应该尽快服务最紧急的请求。具体来说，我们通过如下公式选择最紧急的请求 $n^*$：

    $$
    n^* = \arg\min_n \left( \min \left\{ \xi^p_{g,t,n}, \xi^d_{g,t,n} \right\} \right)
    $$

    该公式确保在所有可选择的请求中，选择那个具有最小紧急度的请求，即最需要立即处理的请求。
    在确定了最紧急的请求后，需要进一步检查该请求是否在当前情况下是有效的行动，即该请求的接取或送达是否在有效的动作空间内。如果 $n^*$ 的接取（$$p_{n^*}$$）或送达（$$d_{n^*}$$）动作不是有效动作（即不在 $$\hat{A}_{g,t,k}$$ 中），那么车辆 $k$ 将从有效动作空间中随机选择一个动作。
    改进后的 $\epsilon$-贪心策略的动作选择函数如下：

    $$
    \Gamma(o_{g,t,k}) = 
    \begin{cases}
    p_{n^*}, & \text{if } \xi^p_{g,t,n} \neq \mathcal{M} \text{ and } \xi^p_{g,t,n} < \xi^d_{g,t,n} \text{ and } p_{n^*} \in \hat{A}_{g,t,k} \\
    d_{n^*}, & \text{if } \xi^d_{g,t,n} \neq \mathcal{M} \text{ and } \xi^d_{g,t,n} < \xi^p_{g,t,n} \text{ and } d_{n^*} \in \hat{A}_{g,t,k} \\
    \text{Random action from } \hat{A}_{g,t,k}, & \text{otherwise}
    \end{cases}
    $$

    如果最紧急的请求有效，车辆优先处理最紧急的动作；否则从有效动作空间中随机选择。
    + **双重探索$\epsilon$-贪心策略**
      + 引入两个初始探索率 $\epsilon_1$ 和 $\epsilon_2$，确保在训练初期有足够的随机性以探索新的可能性。其中 $\epsilon_1 + \epsilon_2 \leq 1$，随训练进步，这两个探索率会按某种机制逐渐衰减。
        - 概率 $\epsilon_1(\tau)$：执行完全随机动作，来自有效动作空间 $\hat{A}_{g,t,k}$。
        - 概率 $\epsilon_2(\tau)$：根据时间相关信息选择动作，即选择处理最紧急的请求或是当前最优的动作。
        - 概率 $1 - \epsilon_1(\tau) - \epsilon_2(\tau)$：选择当前估计回报最大的贪心动作，也就是argmaxQ动作。

        $$
        a^{\mathcal{M}}_{g,t,k} = 
        \begin{cases} 
        \text{Random action from } \hat{A}_{g,t,k}, & \text{with probability } \epsilon_1(\tau) \\
        \Gamma(o_{g,t,k}), & \text{with probability } \epsilon_2(\tau) \\
        \text{argmaxQ}(o_{g,t,k}, a_{g,t,k}), & \text{with probability } 1 - \epsilon_1(\tau) - \epsilon_2(\tau)
        \end{cases}
        $$

        其中，$argmaxQ(o_{g,t,k}, a_{g,t,k})$ 表示选择当前能带来最大预期回报的贪心动作。

  + **邻域搜索**
    强化学习生成的初始解决方案通常未必是最优的，局部搜索通过在初始解的邻域内进行搜索来尝试找到更好的解决方案。局部搜索的核心在于设计有效的局部搜索算子，这些算子能够在当前解的邻域内执行移动操作，如交换、插入或逆转等，以探索可能的改进。
    
    邻域操作类型：**内环移动（Intra-loop move）**、**跨环移动（Inter-loop move）**、**插入被拒绝请求（Insert-rejected-request move）**
    在通过邻域搜索算子，得到改进后的解 $x$ 后，将其转化为与强化学习（RL）类似的体验，以便进一步优化。
    
  + **损失函数**
    $$TD_{\text{RL}}$$ 的计算公式为：

    $$
    TD_{\text{RL}} = R_{g,t} + \gamma \sum_{k=1}^{|K|} \max_{a \in A_{g,t+1,k}} Q_k(o_{g,t+1,k}, a, \theta_g') - \sum_{k=1}^{|K|} Q_k(o_{g,t,k}, a_{g,t,k}, \theta_g)
    $$

    其中，$R_{g,t}$ 是在时刻 $t$ 的奖励，$\gamma$ 是折扣因子，$Q_k$ 是针对每个智能体的动作值函数，$\theta_g'$ 是目标网络的参数，用于计算下一个状态的最大预期奖励。
    另外，还引入了模仿学习来提高强化学习的训练性能，模仿学习是使用局部搜索算法改进的解决方案来优化损失函数设计。

  + **训练数据和损失函数评估**
    - 从局部搜索缓冲区中采样一批轨迹，轨迹数据为：

      $$e^{LS} = (o^{LS}_{g,t}, A^{LS}_{g,t}, R^{LS}_{g,t}, o^{LS}_{g,t+1}, A^{LS}_{g,t+1})$$
    
    - 计算每个轨迹的时间差分误差（TD-error），并通过平均平方误差来评估模型的表现：

      $$
      L^{IL} = \frac{1}{|a^{LS}|} \sum_{s=1}^{|a^{LS}|} \left[ \sum_{k=1}^{|K|} \left( \max_{a \in A^{LS}_{g,t,k}} Q_k(o^{LS}_{g,t,k}, a, \theta_g) + \delta(s^{LS}_{g,t,k}, a^{LS}_{g,t,k}) - Q_k(o^{LS}_{g,t,k}, a^{LS}_{g,t,k}, \theta_g) \right)^2 \right]
      $$

      其中，$$\delta(s^{LS}_{g,t,k}, a^{LS}_{g,t,k})$$ 是一个边缘函数，用来避免损失函数的零值问题。当预测的动作是最大值动作时，这个边缘值为 0.8，否则为 0。

    - 总损失函数是 TD-error 损失 $L^{TD}$ 和模仿学习损失 $L^{IL}$ 的组合：

      $$
      L = L^{TD} + L^{IL}
      $$

+ **数值实验**
  - **网络布局**：Sioux Falls网络，包含24个站点，其中1个作为仓库，其他23个作为中转站。
  - **时间和需求**：对于单周期问题，随机生成需求，考虑了起点和终点在服务站点之间的随机分布。
  - **多周期问题**：需求生成遵循泊松分布，考虑动态场景下从一个站点到另一个站点的需求变化。
  - **时间间隔**：每个周期包含30个时间间隔。
  - **车速和成本系数**：设定了单位运输成本、迟到惩罚成本和其他相关成本参数。
  - **奖励权重**：设置了关于成本和时间的奖励权重。
  - **预测精度**：描述了预测请求与实际请求之间的不一致性，并设置了预测的时间长度。

+ **个人总结**
这篇文章相当完善，工作量上很饱和，并且也有一些设计上的巧思，比如MDP状态、探索方法的设计等，是一篇值得发表在TRB上的论文。但是，读起来感觉作者一开始并不是直接做的强化学习，而是想采用组合优化方法和滚动时域框架去通过传统数学优化方法求解这一问题，因此一些地方看起来会有点奇怪。但作者采用启发式方法和模仿学习结合起来去指导强化学习算法的训练这一点还是值得学习的。

另外，这篇文章存在着非常多没必要的叙述，有点为了让文章显得复杂而这么写，去迎合TRB期刊特点的意思。