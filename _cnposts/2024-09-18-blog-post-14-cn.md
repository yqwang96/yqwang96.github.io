---
title: 'LangChain多知识库同时检索时filter不生效BUG的解决办法'
date: 2024-09-18
permalink: /cnposts/2024/09/blog-post-14/
tags:
  - Problem analysis
  - Code-sharing
---


最近业务上有个RAG知识库管理开发的需求，组里在技术选型上选定的基于langchain和langserve进行开发。主要的技术需求有：
+ 实现机器人（Agent）对多个知识库内容的同步检索；
+ 实现机器人（Agent）在多个知识库之间的检索切换；

对于上面这两点需求，参考Langchain的文档，打算基于langchain中的milvus接口类构建retriever，每个retriever连接到milvus中不同的collection（相当于数据库中不同的表），再进行invoke的时候，传入针对知识库检索的参数，进行过滤，从而能够实现多知识库同步检索，以及检索切换功能（通过传入参数限制为单一知识库检索，近似实现检索切换）。

为此，在编写代码的时候，参考 https://python.langchain.com/v0.2/docs/how_to/qa_per_user/ 的示例。在创建Retriever指定能够配置的参数，然后在invoke的时候再去传入这个参数，进而实现筛选。示例代码如下：

```python
vectorstore = Milvus(connection_args=milvus_config, embedding_function=embedding_model, collection_name='default')

test_retriever = vectorstore.as_retriever().configurable_fields(
    search_kwargs=ConfigurableField(
        id="search_kwargs",
        name="doc's search_kwargs",
        description="doc's search_kwargs",
    )
)


template = """Answer the question based only on the following context:
{context}
Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

rag_chain = ({"context": test_retriever, 'question': RunnablePassthrough()} | prompt | llm)

rag_chain.invoke('What impact does the education level of older workers have on the labor market?',
                 config={"configurable": {'search_kwargs': {'expr': "namespace == 'wang'"}}})
```

通过上述代码，可以根据向量数据库中数据的metadata信息进行筛选，如上述代码筛选namespace为’wang‘的信息。但是，因为官方是以PINECONE向量数据库为例的，当用到milvus中时，需要根据Langchain已经实现的milvus接口类中的输入参数进行筛选，比如在上述代码中是根据`expr`进行筛选的。

通过上面的代码，结合langchain中多retriever进行ensemble的功能，能够实现多个retriever的同步检索和切换。那么问题就此解决了吗？并没有。

业务上还需要对外提供langserve的接口，因此需要把上述功能封装为一个Langchain的Runnable子类，然后重写invoke方法，并通过langserve对外提供接口。为此，把上述代码封装为WebChain类，并交给langserve后，发现对invoke进行相同的输入配置参数，并不能生效。

```python
# 服务端代码
from typing import Optional, Any
from langchain.retrievers import EnsembleRetriever
from langchain_core.runnables import ConfigurableField
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnablePassthrough, RunnableConfig, Runnable, RunnableMap
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.utils import Input, Output
from langchain_milvus import Milvus


class WebChain(Runnable):
    def __init__(self):
        super().__init__()
        self.milvus_config = {
            "host": "localhost",
            "port": 19530,
            "db_name": "default"
        }

        self.llm = YourLLM

        self.embeddings = YourEmbedding
        
    def get_chain(self):
        collection_name_list = ['Default_0', 'Default_1']  # 从数据库中读取配置

        vectorstore_list = [
            Milvus(
                connection_args=self.milvus_config, embedding_function=self.embeddings, collection_name=collection_name
            ) for collection_name in collection_name_list
        ]

        retriever_list = [single_vectorstore.as_retriever().configurable_fields(
            search_kwargs=ConfigurableField(
                id="search_kwargs_0",
                name="doc's search_kwargs",
                description="doc's search_kwargs"
            )
        ) for single_vectorstore in vectorstore_list]

        ensemble_retriever = EnsembleRetriever(
            retrievers=retriever_list, weights=[1 / len(retriever_list) for _ in range(len(retriever_list))]
        )

        template = """Answer the question based only on the following context:
        {context}
        Question: {question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = {"context": ensemble_retriever, 'question': RunnablePassthrough()} | prompt | self.llm
        return chain

    def invoke(
        self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    ) -> Output:
        chain = self.get_chain()
        mes = chain.invoke(input, config=config)
        return mes
```

通过打印服务端的信息发现，client发送config在服务端完全没有收到。检查langserve源代码发现，langserve会对传入的config做一系列处理，然后得到最终的configurable。具体细节在这里我没有研究。但最终是通过langserve中一个示例来解决。归纳起来是，把传入的参数放到http requests的headers中。服务端拿到headers数据后，做处理，并传入到configurable中,如下所示

```python
# Client端
import requests
from langchain.globals import set_verbose, set_debug

set_verbose(True)
set_debug(True)

query = "What impact does the education level of older workers have on the labor market?"

url = f"http://localhost:8001/rag/invoke"

json1 = {
    'input': query,
}
headers = {"search_kwargs_0": "collection_name == 'Default_1'"}

response = requests.post(url, json=json1, headers=headers)

# 打印返回的结果
if response.status_code == 200:
    print("Response:")
    print(response.json())
else:
    print(f"Failed with status code: {response.status_code}")
    print(response.text)

# 服务端
import uvicorn
from typing import Dict, Any
from WebChain import WebChain
from langserve import add_routes
from fastapi import FastAPI, HTTPException, Request


def fetch_api_key_from_header(config: Dict[str, Any], req: Request) -> Dict[str, Any]:
    if "search_kwargs_0" in req.headers:
        config["configurable"]["search_kwargs_0"] = {'expr': req.headers["search_kwargs_0"]}
    return config


chain = WebChain()

app = FastAPI(
    title="LangChain Server",
    version="1.0",
    description="A simple API server using LangChain's Runnable interfaces",
)

add_routes(
    app,
    chain,
    path="/rag",
    per_req_config_modifier=fetch_api_key_from_header
)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

但是,由于headers中无法直接发送json.这种方法需要把配置以string的形式传递,然后在fetch_api_key_from_header函数中再处理为所需要的形式.这是这一方法的一个缺点。

------
