---
title: 'Ragflow对接Langchain和Milvus的踩坑日记'
date: 2024-09-22
permalink: /cnposts/2024/09/blog-post-16/
tags:
  - Bug analysis
  - Code-sharing
---

围绕着先前所提到的RAG知识库管理的开发需求，在上周五技术方案讨论会后，还是决定先尝试能否把已有的LangChain代码整合到ragflow的接口内，这样能够减少开发需求，避免重复造轮子。今天，在家里研究了一天，踩了一些坑，算是写了个demo，跑通了整体的流程。现在做一个记录，供有同样需求的朋友们参考；同时，也便于日后回顾。

在此对Ragflow和LangChain先不做过多介绍。感兴趣的朋友可以参考[RAGFlow Github仓库](https://github.com/infiniflow/ragflow)和[LangChain文档](https://python.langchain.com/v0.2/docs/introduction/)。说回现在的开发需求，目前已经实现了采用Langserve场景和不采用Langserve场景的知识库切换管理功能，接下来的任务是要将这部分功能整合到ragflow内。

在我研究基于langserve实现时，是在docker里拉取milvus镜像，然后创建容器进行测试的。而我的ragflow也是通过docker拉取镜像、创建容器部署的。所以，如果想要将Ragflow和Langchain进行整合，那么需要将这两个镜像组整合到一起。这是踩的第一个坑。

## Ragflow与milvus环境整合

在整合Ragflow与Milvus环境的过程中，遇到的一个关键问题是如何处理两个镜像组内存在的MinIO镜像重复问题。初步方案是通过编写一个单独的Dockerfile，将两个镜像组整合在一起，并且只保留Milvus镜像中的MinIO以避免重复。

虽然成功拉取了整合后的镜像并创建了容器，但在运行时发现了一个严重问题：代码无法连接到Milvus，无法完成向量化的数据入库，前端的向量知识库可视化软件也无法生效。虽然Milvus容器能够启动，但每次尝试连接时容器会自动关闭。即便查看日志，依然没有发现明确的报错信息。

### 排查过程
起初，怀疑是MinIO或etcd的配置问题，导致Milvus无法与这两个组件正确通信，从而触发了自动关闭机制。对此进行了多次配置调整，但依然未能解决问题。由于难以找到明确的错误原因，最终放弃了这一整合思路。

### 转换思路
转而采取另一种方案：分别拉取Milvus和Ragflow的镜像，停掉Ragflow中的MinIO服务，仅运行Milvus镜像中的MinIO。在此方案中，重点是确保Ragflow和Milvus能够在同一个网络中正常通信。为此，在Docker Compose文件中需要明确指定两个容器共享同一个网络，如下所示：

```yml
version: '3'
services:
  milvus-standalone:
    image: milvusdb/milvus-standalone:latest
    container_name: milvus
    networks:
      - shared_network
    environment:
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=your_access_key
      - MINIO_SECRET_KEY=your_secret_key
    ports:
      - "19530:19530"
      - "9000:9000"

  ragflow-server:
    image: your_ragflow_image
    container_name: ragflow
    networks:
      - shared_network
    depends_on:
      - milvus-standalone
    environment:
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=your_access_key
      - MINIO_SECRET_KEY=your_secret_key
    ports:
      - "8080:8080"

networks:
  shared_network:
    driver: bridge
```

## Ragflow聊天接口调用流程

在测试Ragflow聊天接口调用时，流程如下：

1. 首先查看Ragflow容器的调用日志，发现发送消息后是通过一个名为`completion`的接口进行调用。接着在PyCharm中搜索这个接口。
2. 找到该接口后，发现它调用了`dialog_service`中的`Chat`接口。因此，下一步的需求是在messenger中获取用户的问题，并通过`Chat`接口返回Ragflow所要求的格式。
3. 在后续过程中，需要将代码粘贴进去，并通过`pip install`安装一些必要的库。另外，在调试时，使用`print`语句无法输出调试信息。为了解决这个问题，可以利用`chat_logger`变量，通过`chat_logger.error`记录相关信息，将这些错误信息保存到TXT文件中，以便查看每一步的执行情况。

## 详细调试流程

在明确了上一部分后，如果想要进行后续调试，首先需要将文本入库。通过本地访问暴露的端口来进行入库。最初，我使用`huggingfaces/sentence-transformer`进行入库。同时，在Retriever中也采用这个embedding模型对query进行向量化，从而提取出相似文本。

问题在于，这个模型是通过本地加载的，尽管模型的参数量不算很大，但加载过程仍需要时间。这会导致Ragflow后端在执行时卡住。为了解决这个问题，想到可以调用一些成熟的API，因为网络接口不需要再部署执行，可以更高效地进行embedding。

在后续执行时，因为我只修改了Retriever中的embedding模型，而在入库时并未更新embedding模型，这导致了部分错误。因此，需要针对新采用的embedding模型重新进行向量化入库。在向量化入库后，通过在Ragflow中不断打日志来观察每一部分变量的状态，以便调试。

经过多次修改response格式，最终使Ragflow能够正确返回我们提取的数据文本。对于知识库的筛选，可以采用最初不使用langserve的With config方法进行筛选。

如果后期需要进一步筛选，可以通过Ragflow的dialog中的知识库ID从数据库中读取相关信息，构造config的JSON，最终传递过去以实现筛选和回复。

整体流程如上。详细代码可参考下述内容。


```python
def chat(dialog, messages, stream=True, **kwargs):
    chat_logger.error('test chat!!!!!!!!!!!!!!!!!!!!!!!')
    import os
    import getpass
    from langchain_community.embeddings import ZhipuAIEmbeddings
    from typing import Optional, Any, Type, List, Dict
    from langchain.retrievers import EnsembleRetriever
    from langchain_core.runnables import ConfigurableField
    from langchain_openai import ChatOpenAI
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_core.runnables import RunnableConfig, Runnable, RunnableBinding, RunnablePassthrough
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables.utils import Input, Output
    from langchain_milvus import Milvus

    config = {
    'configurable': {
         "search_kwargs_0": {
             "expr": f"collection_name in ['Default_1']"
         }
     }
    }

    if not os.getenv("ZHIPUAI_API_KEY"):
        os.environ["ZHIPUAI_API_KEY"] = 'xxx.xxx'

    milvus_config = {
        "host": "172.19.0.8",
        "port": 19530,
        "db_name": "default"
    }

    llm = ChatOpenAI(
        model='deepseek-chat', openai_api_key='sk-xxxx',
        openai_api_base='https://api.deepseek.com', max_tokens=4096
    )
    chat_logger.error('connect_llm!!!!!!!!!!!!!!!!!!!!!!!')
    embeddings = ZhipuAIEmbeddings(
        model="embedding-3",
    )
    chat_logger.error('load embedding!!!!!!!!!!!!!!!!!!!!!!!')
    collection_name_list = ['Default_0', 'Default_1']
    chat_logger.error('read collection name list')

    vectorstore_list = [
        Milvus(
            connection_args=milvus_config, embedding_function=embeddings, collection_name=collection_name
        ) for collection_name in collection_name_list
        ]
    chat_logger.error('success connect Milvus!!!!!!!!!!!!!!!!!!!')

    retriever_list = [single_vectorstore.as_retriever().configurable_fields(
        search_kwargs=ConfigurableField(
            id="search_kwargs_0",
            name="doc's search_kwargs",
            description="doc's search_kwargs"
        )
    ) for single_vectorstore in vectorstore_list]
    chat_logger.error('build retriever success!!!!!!!')

    ensemble_retriever = EnsembleRetriever(
        retrievers=retriever_list, weights=[1 / len(retriever_list) for _ in range(len(retriever_list))]
    )
    chat_logger.error('ensemble success!!!!!!!!!!')

    template = """Answer the question based only on the following context:
            {context}
            Question: {question}
            """
    prompt = ChatPromptTemplate.from_template(template)
    chain = {"context": ensemble_retriever, 'question': RunnablePassthrough()} | prompt | llm

    questions = [m["content"] for m in messages if m["role"] == "user"][-1]
    chat_logger.error(f'questions:{questions}!!!!!!!!')

    try:
        mes = chain.invoke(str(questions), config=config)
    except Exception as e:
        chat_logger.error(str(e))

    chat_logger.error(f'mes content:-------------!!!!----!!!\n{str(mes)}')
    result = {"answer": str(mes).split("'")[1], "reference": [], "prompt": ""}

    chat_logger.error(f'llm_results!!!!!!!!!!!!!!!!{result}')
    yield result
    return result
```


---

